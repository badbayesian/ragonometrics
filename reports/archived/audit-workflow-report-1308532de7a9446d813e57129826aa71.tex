% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\section{\texorpdfstring{Audit Report: Workflow
\texttt{1308532de7a9446d813e57129826aa71}}{Audit Report: Workflow 1308532de7a9446d813e57129826aa71}}\label{audit-report-workflow-1308532de7a9446d813e57129826aa71}

\subsection{Overview}\label{overview}

\begin{itemize}
\tightlist
\item
  Source JSON:
  \texttt{reports\textbackslash{}workflow-report-1308532de7a9446d813e57129826aa71.json}
\item
  Run ID: \texttt{1308532de7a9446d813e57129826aa71}
\item
  Papers input:
  \texttt{papers\textbackslash{}Use\_of\_Cumulative\_Sums\_of\_Squares\_for\_Re.pdf}
\item
  Started at: \texttt{2026-02-15T17:40:01.994514+00:00}
\item
  Finished at: \texttt{2026-02-15T17:53:00.314525+00:00}
\item
  Duration: \texttt{0:12:58.320011}
\end{itemize}

\subsection{Effective Configuration}\label{effective-configuration}

\begin{itemize}
\tightlist
\item
  Chat model: \texttt{gpt-5}
\item
  Embedding model: \texttt{text-embedding-3-large}
\item
  Top K: \texttt{10}
\item
  Chunk words / overlap: \texttt{350} / \texttt{75}
\item
  Batch size: \texttt{64}
\item
  Database URL configured: \texttt{True}
\end{itemize}

\subsection{Step Outcomes}\label{step-outcomes}

\begin{itemize}
\tightlist
\item
  \texttt{prep}: \texttt{completed}
\item
  \texttt{ingest}: \texttt{n/a}
\item
  \texttt{enrich}: \texttt{n/a}
\item
  \texttt{econ\_data}: \texttt{fetched}
\item
  \texttt{agentic}: \texttt{completed}
\item
  \texttt{index}: \texttt{skipped} (reason: \texttt{db\_unreachable})
\item
  \texttt{evaluate}: \texttt{n/a}
\item
  \texttt{report\_store}: \texttt{skipped} (reason:
  \texttt{db\_unreachable})
\end{itemize}

\subsection{Agentic Summary}\label{agentic-summary}

\begin{itemize}
\tightlist
\item
  Status: \texttt{completed}
\item
  Main question: What is the key contribution?
\item
  Report question set: \texttt{structured}
\item
  Structured questions generated: \texttt{83}
\item
  Confidence mean/median: \texttt{0.27249828745446014} /
  \texttt{0.268446744281151}
\item
  Confidence labels: low=77, medium=6, high=0
\end{itemize}

\subsubsection{Final Answer}\label{final-answer}

\begin{itemize}
\item
  Primary: Introduces the ICSS procedure for variance change-point
  detection---built on a statistic \(D_k\) that is a monotone transform
  of the two-sample variance F statistic
  \(F_{T-k,k}=\frac{(C_T-C_k)/(T-k)}{C_k/k}\) with
  \(C_k=\sum_{t=1}^k a_t^2\)---and locates breaks by maximizing
  \(\max_k |D_k|\); under homogeneity, the \(D_k\) process converges to
  a Brownian bridge, enabling use of fixed critical values (e.g., 5\% ≈
  1.358) for inference {[}page 3 words 0--349; page 12 words 0--349{]}.
\item
  Supporting:

  \begin{itemize}
  \tightlist
  \item
    Provides asymptotic moments under no breaks (e.g.,
    \(E[C_k]=k\sigma^2\), \(E[C_k C_T]=k(T+2)\sigma^4\)) and shows
    \(E[D_k]\to 0\) as \(T\) grows, justifying the null behavior used
    for thresholding {[}page 11 words 275--507{]}.
  \item
    Benchmarks ICSS against likelihood-ratio and Bayesian posterior-odds
    approaches via extensive simulations (various \(T\), change
    locations, and variance ratios), showing strong detection except
    when changes are very early and small; performance improves with
    larger \(T\) and variance ratios {[}page 7 words 0--349; page 7
    words 275--556; page 9 words 550--899{]}.
  \item
    Establishes ICSS as a computationally light alternative with stable,
    lower CPU cost versus LR (roughly \(O(T^3)\)), especially beneficial
    for long series and multiple changes {[}page 9 words 825--904; page
    11 words 275--507{]}.
  \item
    Extends application to AR(1) settings via prewhitened residuals,
    yielding results close to the i.i.d. case {[}page 9 words 550--899;
    page 10 words 0--84{]}.
  \item
    Outlines complementary testing frameworks: sequential LR tests for
    the number of change points with extreme-value (Gumbel)
    standardization, and Bayesian posterior odds with per-time change
    probability, to compare modeling strategies {[}page 8 words
    0--329{]}.
  \end{itemize}
\end{itemize}

\subsubsection{Sub-Answers}\label{sub-answers}

\paragraph{Sub-answer 1}\label{sub-answer-1}

\begin{itemize}
\tightlist
\item
  Question:
  ResponseTextConfig(format=ResponseFormatText(type=\textquotesingle text\textquotesingle),
  verbosity=\textquotesingle medium\textquotesingle)
\item
  Answer:
\end{itemize}

Could you clarify what you'd like from these excerpts (e.g., statistic
definitions, testing procedure, simulation setup/results, or
computational complexity)? Key points available here include:

\begin{itemize}
\item
  ICSS statistic and F-test link

  \begin{itemize}
  \tightlist
  \item
    For a fixed k, the usual two-sample variance F statistic is
    \(F_{T-k,k} = \frac{(C_T - C_k)/(T-k)}{C_k/k}\), where
    \(C_k=\sum_{t=1}^k a_t^2\); \(D_k\) is a monotone function of
    \(F_{T-k,k}\) and depends on k only via \(k/T\). Unlike the F-test
    (known k), ICSS searches \(\max_k |D_k|\) to locate a change point
    {[}page 3 words 0--349{]}.
  \item
    Asymptotically (Normal noise), the \(D_k\) process is a Brownian
    bridge; an example uses the 5\% critical value 1.358 with detection
    at \(k^*=342\) when \(\max_k|D_k|\) exceeds this boundary {[}page 3
    words 0--349{]}.
  \end{itemize}
\item
  Asymptotics and moments under homogeneity

  \begin{itemize}
  \tightlist
  \item
    Under \(N_T=0\): \(E[C_k]=k\sigma^2\),
    \(E[C_k C_T]=k(T+2)\sigma^4\), and \(E[D_k]\to 0\) as \(T\) grows
    {[}page 11 words 275--507{]}.
  \item
    Appendix sketch: with \(t_i=a_i^2-\sigma^2\), a Donsker-type
    argument yields convergence to a Brownian bridge \(W^0\) {[}page 12
    words 0--349{]}.
  \end{itemize}
\item
  Likelihood ratio (LR) testing for number of change points

  \begin{itemize}
  \tightlist
  \item
    Strategy: sequentially test \(H_0:N_T=n-1\) vs \(H_a:N_T=n\) using
    \(LR_{n-1,n}\) for \(n=1,2,\dots\); critical values require
    simulation because the problem is nonregular {[}page 8 words
    0--329{]}.
  \item
    Standardize \(LR_{n-1,n}\) to reference the extreme value limit with
    cdf \(F(x)=\exp(-e^{-x})\); formulas for standardization are given
    in Table 3 (referenced) {[}page 8 words 0--329{]}.
  \end{itemize}
\item
  Bayesian posterior odds (multiple change points)

  \begin{itemize}
  \tightlist
  \item
    Posterior odds compare \(p(N_T=n\mid a)\) across n, using
    \(p(a,K\mid N_T)\) and a per-time prior change probability \(X\);
    details and Table 5 summarize two-change cases {[}page 8 words
    0--329{]}.
  \end{itemize}
\item
  Simulation design (comparisons: ICSS, LR, Bayesian)

  \begin{itemize}
  \tightlist
  \item
    1,000 replicates per design; \(T\in\{100,200,500\}\); one-change
    locations \(K/T\in\{.25,.5,.75\}\); variance ratios \(A\in\{2,3\}\).
    Two-change locations
    \((K_1/T,K_2/T)\in\{(.33,.66),(.20,.80),(.58,.80)\}\) with
    permutations of \((\sigma_1^2,\sigma_2^2,\sigma_3^2)\) constrained
    by \(A_1=\sigma_2^2/\sigma_1^2\), \(A_2=\sigma_3^2/\sigma_2^2\) and
    symmetry reductions. Data are iid \(N(0,1)\); Fortran
    implementation; RNG via Bratley--Fox--Schrage; Sparcstation 1+
    {[}page 7 words 0--349; page 7 words 275--556{]}.
  \end{itemize}
\item
  Main empirical findings (one change)

  \begin{itemize}
  \tightlist
  \item
    With \(T=100\) and small variance ratio (\(A=2\)), detection is
    hard---especially if the change is near the beginning---with ICSS.
    With \(T=200\) or \(A=3\), correct identification exceeds 80\% when
    the change is near the middle. ICSS improves with larger T and A.
    The Bayesian approach performs best for \(A=2\) and is comparable to
    others for \(A=3\) {[}page 9 words 550--899{]}.
  \end{itemize}
\item
  AR(1) residual application

  \begin{itemize}
  \tightlist
  \item
    Generate \(Y_t=\phi Y_{t-1}+a_t\); apply ICSS to residuals
    \(\varepsilon_t=Y_t-\hat\phi Y_{t-1}\). The percentage with
    \(N_T=1\) is close to the iid results in Table 4 (see Table 8)
    {[}page 9 words 550--899; page 10 words 0--84{]}.
  \end{itemize}
\item
  Computational burden

  \begin{itemize}
  \tightlist
  \item
    ICSS was designed to avoid the heavy computation of posterior odds
    and the need to evaluate functions at all possible change locations.
    LR uses Baufays--Rasson's algorithm for maximizing the likelihood
    but remains heavy for long series and multiple changes {[}page 9
    words 825--904{]}.
  \item
    CPU-time scaling: LR is roughly \(O(T^3)\) and of similar order for
    \(N_T=2\) and \(N_T=3\); ICSS's dependence on T is the same across
    values of \(N_T\) (lower and more stable cost) {[}page 11 words
    275--507{]}.
  \end{itemize}
\end{itemize}

\subsection{Structured Q\&A Appendix}\label{structured-qa-appendix}

This section mirrors \texttt{agentic.report\_questions} for audit
traceability.

\subsubsection{A01: What is the main research question of the
paper?}\label{a01-what-is-the-main-research-question-of-the-paper}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.25738287842443736)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

To develop and assess a method---based on cumulative sums of squares
(ICSS)---for retrospectively detecting changes in variance in time
series, including determining the number and locations of variance
changepoints, and to compare its performance with likelihood ratio and
Bayesian approaches.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The performance of different procedures used to determine the number of
variance changes in a series
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-120, section=4. SIMULATION EXPERIMENT, note=States
    the study evaluates procedures to determine the number of variance
    changes and compares ICSS with likelihood ratio and Bayesian
    methods.
  \item
    page=4, words=0-40, section=Inclán and Tiao: Retrospective Detection
    of Changes in Variance, note=Section header emphasizes focus on
    retrospective detection of variance changes.
  \item
    page=6, words=550-640, section=ICSS algorithm application,
    note=Describes the ICSS procedure as a systematic search for points
    of change.
  \end{itemize}
\end{itemize}

\subsubsection{A02: What is the paper\textquotesingle s primary
contribution relative to the existing
literature?}\label{a02-what-is-the-papers-primary-contribution-relative-to-the-existing-literature}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.27419868133310066)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{direct\ textual\ evidence}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

It introduces the ICSS algorithm---a centered cumulative-sum-of-squares,
iterative method---to retrospectively detect multiple variance change
points efficiently; it develops properties of the centered CSS statistic
and shows via simulations that ICSS matches Bayesian/likelihood-ratio
performance for moderate samples without their heavy computational
burden.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
We propose a procedure to detect variance changes based on an iterated
cumulative sums of squares (ICSS) algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=States proposal of
    ICSS, study of centered CSS properties, and comparable performance
    without heavy computation.
  \item
    page=2, words=275-624, section=Introduction, note=Describes using
    centered cumulative sums of squares and an iterative algorithm to
    find multiple change points.
  \end{itemize}
\end{itemize}

\subsubsection{A03: What is the central hypothesis being
tested?}\label{a03-what-is-the-central-hypothesis-being-tested}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.25484008361822535)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Answer:
\end{itemize}

H0: no change in variance (NT = 0) versus H1: one change in variance (NT
= 1).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
testing one change against no change in variance
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=300-430, section=Section 2.1, note=Defines NT=0 (no
    change) and NT=1 (one change) and states testing one change against
    no change in variance.
  \item
    page=7, words=310-400, section=Section 4.1, note=Defines LR\_mn for
    testing H0: NT = m against Ha: NT = n (number of variance changes).
  \end{itemize}
\end{itemize}

\subsubsection{A04: What are the main outcomes of interest (dependent
variables)?}\label{a04-what-are-the-main-outcomes-of-interest-dependent-variables}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.26037546805984413)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{explicit\ statement}
\item
  Data source: Section 4. SIMULATION EXPERIMENT description
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The main outcomes are the number of correct identifications of variance
changes and the computational time (CPU time).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
the main way is by the number of \textquotesingle correct
identifications.\textquotesingle{} Another aspect to consider is the
computational requirements of each approach, which can be measured in
terms of the CPU time.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=Defines
    primary performance measures: correct identifications and CPU time.
  \end{itemize}
\end{itemize}

\subsubsection{A05: What are the key treatment/exposure variables
(independent
variables)?}\label{a05-what-are-the-key-treatmentexposure-variables-independent-variables}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.2180384234455463)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{methods/simulation\ design\ description}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Series length (T); location(s) of changepoint(s) (K or K1, K2); variance
ratio(s) (A for one change; A1 = τ1/τ0 and A2 = τ2/τ1 for two changes);
configuration/permutation of segment variances; and the number of
changepoints (NT).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
For Part One, we used three series lengths ( T = 100,200, and 500),three
locations of the changepoint ( K , = .25T, .50T, and . 7 5 T ) ,and two
values for the variance ratio ( A = 2 and 3 ) . For Part Two, we used
the same three series lengths ... three different sets oflocations ofthe
change points ... and six possible configurations of variances ... keep
the corresponding ratios A, = T : / T \textasciitilde{} and A2 = T ; / T
; .
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=page 7, words=0-349, section=Simulation Experiment (Part One,
    Part Two), note=Enumerates manipulated factors: T, K (or K1,K2), A
    (or A1,A2), and variance configurations.
  \end{itemize}
\end{itemize}

\subsubsection{A06: What setting/context does the paper study (country,
market,
period)?}\label{a06-what-settingcontext-does-the-paper-study-country-market-period}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.29809896205083153)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{direct\_text}
\item
  Data source: Box and Jenkins (1976)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Applied example: IBM stock closing prices (first differences of log
prices) over May 17, 1961 to November 2, 1962; country not specified in
the provided text.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices from May 17, 1961, to November 2, 1962, as reported
by Box and Jenkins (1976).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=360-430, section=3.1 Example: IBM Stock Prices,
    note=States the data series and its time span.
  \end{itemize}
\end{itemize}

\subsubsection{A07: What is the main mechanism proposed by the
authors?}\label{a07-what-is-the-main-mechanism-proposed-by-the-authors}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.23553237245887174)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{direct\ textual\ evidence}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The Iterated Cumulative Sums of Squares (ICSS) algorithm---an iterative
use of the centered cumulative sum of squares (Dk) to systematically
locate multiple variance-change points.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
We propose a procedure to detect variance changes based on an iterated
cumulative sums of squares (ICSS) algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction/Abstract, note=States
    proposal of ICSS to detect variance changes.
  \item
    page=2, words=275-624, section=Introduction, note=Describes using
    cumulative sums of squares and iterative algorithm.
  \item
    page=5, words=825-1174, section=ICSS Algorithm description,
    note=Algorithm labeled and illustrated.
  \end{itemize}
\end{itemize}

\subsubsection{A08: What alternative mechanisms are
discussed?}\label{a08-what-alternative-mechanisms-are-discussed}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.242333650293353)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interpreted
  \textquotesingle mechanisms\textquotesingle{} as alternative
  explanations for observed features/regime shifts: heavy-tailed vs
  variance-shift modeling, and variance vs AR-parameter changes.
\item
  Answer:
\end{itemize}

Two alternative mechanisms are discussed: (1) modeling heavy tails in
returns (e.g., Pareto/heavy‑tailed distributions) versus explaining them
by variance shifts (nonstationary variance with step changes), and (2)
changes in variance versus changes in the autoregressive parameter as
drivers of regime shifts.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
A Bayesian analysis that allows both the variance and the autoregressive
parameter to change at the same points ... indicates that there are both
variance changes and parameter changes.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=275-624, section=Introduction, note=Contrasts
    heavy-tailed distributions with a normal model having variance step
    changes.
  \item
    page=6, words=550-899, section=Section 3.1 Example, note=States that
    both variance and autoregressive parameter changes can occur.
  \end{itemize}
\end{itemize}

\subsubsection{A09: What are the main policy implications claimed by the
paper?}\label{a09-what-are-the-main-policy-implications-claimed-by-the-paper}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.2661350049764556)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The paper does not present policy implications; it focuses on developing
and evaluating the ICSS statistical method for retrospectively detecting
variance changes and comparing computational costs.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The main interest is to study the variance of a given sequence of
observations retrospectively, so we can use all the information on the
series to indicate the points of variance change.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=560-650, section=2. CENTERED CUMULATIVE SUMS OF
    SQUARES, note=States the paper's main methodological objective
    (variance change detection).
  \item
    page=6, words=550-640, section=ICSS algorithm application,
    note=Describes the procedure's systematic search for change points.
  \item
    page=7, words=0-120, section=4. SIMULATION EXPERIMENT, note=Frames
    performance comparison of procedures rather than policy discussion.
  \item
    page=9, words=720-870, section=4.6 CPU Time Requirements,
    note=Emphasizes computational burden and efficiency, not policy.
  \end{itemize}
\end{itemize}

\subsubsection{A10: What is the welfare interpretation (if any) of the
results?}\label{a10-what-is-the-welfare-interpretation-if-any-of-the-results}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.2910150337102933)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{False}
\item
  Assumption notes: The provided text discusses statistical detection
  accuracy and computation; it does not introduce any welfare framework.
\item
  Answer:
\end{itemize}

None. The results are statistical---focused on detecting variance
changes, their locations, and computational performance---not on welfare
or economic surplus.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The performance of different procedures used to determine the number of
variance changes in a series can be measured in several ways; the main
way is by the number of "correct identifications."
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=Defines
    performance as correct identifications and CPU time; no welfare
    concepts are introduced.
  \item
    page=9, words=550-899, section=4.2--4.6 Results, note=Discusses
    detection success rates and computational burden; still no welfare
    interpretation.
  \end{itemize}
\end{itemize}

\subsubsection{A11: What are the main limitations acknowledged by the
authors?}\label{a11-what-are-the-main-limitations-acknowledged-by-the-authors}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.2484759032229551)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Direct\ textual\ evidence\ from\ results\ and\ method\ discussion}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

They note several limitations: (1) heavy computational burden for
likelihood/Bayesian methods when searching multiple change points
(posterior-odds runs can be infeasible for long series); (2) a masking
effect with multiple changepoints, where Dk tends to highlight only one;
(3) ICSS struggles with small variance shifts or early changes, and
detecting two monotone-increasing changes often requires large samples;
and (4) the location estimate k* is biased toward the middle of the
series, improving only with larger T or variance ratios.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
it is hard to detect a small variance change (variance ratio A =
2)---particularly when it appears at the beginning of the series and we
are using the ICSS algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=550-899, section=Background/Section 2, note=Heavy
    computational burden when looking for several points of change
    simultaneously
  \item
    page=4, words=0-349, section=Discussion of E{[}Dk{]} with two
    changepoints, note=Masking effect when more than one changepoint
  \item
    page=9, words=550-899, section=Results (4.2, 4.3) and CPU time
    (4.6), note=Hard to detect small/early variance change with ICSS;
    posterior odds infeasible for T=500 (1,358 CPU hours)
  \item
    page=9, words=275-624, section=4.4 Sampling Distribution of k*,
    note=k* distribution skewed and biased toward the middle;
    concentrates with larger T or variance ratio
  \end{itemize}
\end{itemize}

\subsubsection{A12: What does the paper claim is novel about its data or
identification?}\label{a12-what-does-the-paper-claim-is-novel-about-its-data-or-identification}

\begin{itemize}
\tightlist
\item
  Category: \texttt{A)\ Research\ question\ /\ contribution}
\item
  Confidence: \texttt{low} (0.2866564534027284)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{paraphrase\_of\_context}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The paper's novelty is an iterated cumulative sums of squares (ICSS)
algorithm based on a centered cumulative sum of squares statistic to
systematically and retrospectively detect multiple variance
changepoints, achieving performance comparable to Bayesian and
likelihood-ratio methods for moderate samples while avoiding their heavy
computational burden and mitigating masking when multiple changes exist.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
the ICSS algorithm offers results comparable to those obtained by a
Bayesian approach or by likelihood ratio tests, without the heavy
computational burden required by these approaches.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-200, section=Introduction, note=Introduces ICSS and
    claims comparable performance without heavy computational burden.
  \item
    page=2, words=275-410, section=Introduction, note=States use of
    cumulative sums of squares and iterative search for multiple
    changepoints.
  \item
    page=2, words=550-750, section=2. Centered Cumulative Sums of
    Squares, note=Defines the centered and normalized cumulative sum of
    squares statistic Dk used by ICSS.
  \item
    page=6, words=0-140, section=3.1 Example: IBM Stock Prices / ICSS
    illustration, note=Shows ICSS eliminates masking when more than one
    variance change exists.
  \item
    page=9, words=825-900, section=4.6 CPU Time Requirements, note=Notes
    ICSS avoids evaluating functions at all possible changepoint
    locations, reducing computational burden.
  \end{itemize}
\end{itemize}

\subsubsection{B01: What is the identification strategy (in one
sentence)?}\label{b01-what-is-the-identification-strategy-in-one-sentence}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.27741585874335584)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{direct\ description\ from\ text}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Detect variance change points by computing the centered cumulative sum
of squares Dk, flagging k* where \textbar Dk\textbar{} crosses
asymptotic boundaries, then iteratively splitting the series at k* and
reapplying on subsegments with adjacent-point checks to fine-tune until
convergence (ICSS).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
A solution is an iterative scheme based on successive application of Dk
to pieces of the series, dividing consecutively after a possible
changepoint is found.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=825-1174, section=ICSS algorithm steps, note=Iterative
    scheme applying Dk to pieces, dividing after a possible changepoint
    is found.
  \item
    page=6, words=550-899, section=Fine-tuning and convergence,
    note=Systematic search from extremes toward middle; fine-tune by
    checking each point given adjacent ones until convergence.
  \item
    page=3, words=275-624, section=Dk boundaries, note=Dk exhibits
    excursions beyond specified boundaries derived from asymptotic
    distribution under constant variance.
  \item
    page=2, words=0-349, section=Introduction, note=Procedure based on
    an iterated cumulative sums of squares (ICSS) algorithm.
  \end{itemize}
\end{itemize}

\subsubsection{B02: Is the design experimental, quasi-experimental, or
observational?}\label{b02-is-the-design-experimental-quasi-experimental-or-observational}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.26840387625640166)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Data source: Inclan \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Experimental (simulation experiment)

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The simulation experiment has two separate parts, the first for one
changepoint and the second for two changepoints... For each part, we
used 1,000 replicates per design point
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT,
    note=Introduces and describes the simulation experiment design.
  \item
    page=7, words=275-556, section=4.1 and setup details, note=Details
    controlled simulation with replicates and generated data.
  \end{itemize}
\end{itemize}

\subsubsection{B03: What is the source of exogenous variation used for
identification?}\label{b03-what-is-the-source-of-exogenous-variation-used-for-identification}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.3252492401531617)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not applicable. The paper is methodological and does not use exogenous
variation for identification; it detects variance changes via the ICSS
cumulative sums of squares approach.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
We study the detection of multiple changes of variance in a sequence of
independent observations. We propose a procedure to detect variance
changes based on an ICSS algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=States the study
    focuses on detecting multiple variance changes using ICSS, not on
    causal identification.
  \item
    page=2, words=550-899, section=Centered cumulative sums of squares,
    note=Defines the centered cumulative sum of squares used for
    detection.
  \item
    page=3, words=275-624, section=Methodological details,
    note=Discusses testing for a change in variance and likelihood
    ratio---again methodological, not exogenous variation.
  \end{itemize}
\end{itemize}

\subsubsection{B04: What is the treatment definition and
timing?}\label{b04-what-is-the-treatment-definition-and-timing}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.12010050431450066)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{inferred\_from\_context}
\item
  Data source: Inclán \& Tiao (1994), Use of Cumulative Sums of Squares
  for Retrospective Detection of Changes of Variance, JASA 89(427):
  913-923.
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not applicable---the article is a methodological study (ICSS for
detecting variance change points) and does not define a treatment or its
timing.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This article studies the problem of multiple change points in the
variance of a sequence of independent observations.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=States study focus
    on detecting variance change points; no treatment is described.
  \item
    page=11, words=0-349, section=Summary/Appendix, note=Recommends ICSS
    for analyzing series with multiple change points; still no treatment
    definition.
  \end{itemize}
\end{itemize}

\subsubsection{B05: What is the control/comparison group
definition?}\label{b05-what-is-the-controlcomparison-group-definition}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.1971543586339832)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Data source:
  \url{http://links.jstor.org/sici?sici=0162-1459\%28199409\%2989\%3A427\%3C913\%3AUOCSOS\%3E2.0.CO\%3B2-H}
\item
  Answer:
\end{itemize}

There is no traditional control group; performance is compared by
applying alternative procedures (likelihood ratio tests and a Bayesian
approach) alongside ICSS to the same simulated series for paired
comparisons.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The same series is subject to each of the procedures considered to
facilitate paired comparisons between procedures.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=40-140, section=4. SIMULATION EXPERIMENT, note=States
    the same series is subjected to each procedure for paired
    comparisons.
  \item
    page=7, words=0-60, section=4. SIMULATION EXPERIMENT,
    note=Introduces simulation comparing ICSS with likelihood ratio and
    a Bayesian approach.
  \item
    page=2, words=0-120, section=Abstract/Intro, note=Mentions
    simulation results comparing ICSS to other approaches.
  \end{itemize}
\end{itemize}

\subsubsection{B06: What is the estimating equation / baseline
regression
specification?}\label{b06-what-is-the-estimating-equation--baseline-regression-specification}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.2852559821489955)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text\_and\_formula}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: No regression is estimated; the method assumes a
  zero-mean Normal series with piecewise-constant variance and uses the
  centered cumulative sum of squares Dk for detection.
\item
  Answer:
\end{itemize}

Baseline model: a\_t are independent Normal(0, σ\_t\^{}2) with
piecewise-constant variance. Estimating equation/statistic: D\_k =
C\_k/C\_T − k/T, where C\_k = ∑\_\{i=1\}\^{}k a\_i\^{}2; inference is
based on max\_k \textbar D\_k\textbar.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Let Dk = Ck/Cn - kin, where Ck = Cfi=la:.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=60-130, section=Theorem 1, note=Definition of Dk and
    Ck
  \item
    page=3, words=120-200, section=Main text, note=Assumption: \{a\_t\}
    Normally distributed with mean 0 and variances σ\_t\^{}2
  \item
    page=11, words=430-500, section=Main text, note=Under variance
    homogeneity, E{[}Dk{]} → 0
  \item
    page=2, words=860-910, section=Intro/overview, note=Dk oscillates
    around 0 for homogeneous variance
  \end{itemize}
\end{itemize}

\subsubsection{B07: What fixed effects are included (unit, time,
two-way, higher
dimensional)?}\label{b07-what-fixed-effects-are-included-unit-time-two-way-higher-dimensional}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.2763758758257676)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\ absence\ within\ provided\ context}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

None (not applicable). The context discusses variance-change detection
in time series and does not include unit, time, two-way, or
higher-dimensional fixed effects.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This behavior leads to the search for a variance change point via maxk I
Dk I.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=550-882, section=Discussion of Dk for variance change,
    note=Focuses on Dk and detecting variance change points; no mention
    of fixed effects.
  \item
    page=6, words=550-899, section=IBM example and ICSS algorithm,
    note=Applies ICSS to time series variance changes; no fixed-effects
    framework referenced.
  \end{itemize}
\end{itemize}

\subsubsection{B08: What standard errors are used (robust, clustered; at
what
level)?}\label{b08-what-standard-errors-are-used-robust-clustered-at-what-level}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.3315274087668142)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Table/Figure:
  \texttt{Table\ 1\ and\ Table\ 2\ (SE\ formulas\ described\ in\ text)}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Standard errors are Monte Carlo (simulation-based), not robust or
clustered. For quantile estimates they use SE ≈
sqrt{[}p(1−p){]}/(sqrt(n)·f), with n=10,000 and f the estimated density
at the pth quantile; for proportions they use SE = sqrt(p̂(1−p̂)/10,000).
No clustering level applies.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The standard errors were obtained as dp(1 - p)/nf 2, where n = 10,000 is
the number of replicates and f is the density at the pth quantile... The
standard errors in this table are obtained as SE = Vb(l - \$)/ 10,000
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=275-450, section=Discussion of Table 1 quantiles,
    note=SE formula for quantile estimates from simulations
  \item
    page=5, words=450-624, section=Discussion of Table 2 proportions,
    note=SE formula for proportion of series meeting criterion
  \end{itemize}
\end{itemize}

\subsubsection{B09: What is the key identifying assumption (parallel
trends, exclusion restriction,
ignorability)?}\label{b09-what-is-the-key-identifying-assumption-parallel-trends-exclusion-restriction-ignorability}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.2725651917028813)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not specified in the provided context; the document concerns
variance-change detection (ICSS, likelihood ratio) rather than causal
identification, so none of parallel trends, exclusion restriction, or
ignorability is stated.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The performance of different procedures used to determine the number of
variance changes in a series can be measured
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-60, section=4. SIMULATION EXPERIMENT, note=Context
    frames the paper around detecting variance changes, not causal
    inference assumptions.
  \item
    page=4, words=275-350, section=2.3 Asymptotic Behavior of Dk Under
    Homogeneous Variance, note=Asymptotic discussion of Dk (Brownian
    bridge) indicates focus on statistical testing for variance changes.
  \end{itemize}
\end{itemize}

\subsubsection{B10: What evidence is provided to support the identifying
assumption?}\label{b10-what-evidence-is-provided-to-support-the-identifying-assumption}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.2664735476513488)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{theoretical\ +\ simulation}
\item
  Table/Figure: \texttt{Table\ 2}
\item
  Data source: 10,000 Monte Carlo replicates of independent N(0,1)
  series; asymptotic theory (Theorem 1)
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interprets the \textquotesingle identifying
  assumption\textquotesingle{} as (i) the null distribution of Dk being
  Brownian-bridge and (ii) max\textbar Dk\textbar{} locating the true
  variance-change point.
\item
  Answer:
\end{itemize}

They provide both theory and simulations: Theorem 1 shows the centered
cumulative sum of squares converges to a Brownian bridge under
homoscedastic Normal noise, supporting use of Brownian-bridge critical
values; and Monte Carlo evidence (10,000 N(0,1) series) shows the
empirical probability that max\textbar Dk\textbar{} is below the 1.358
critical value is ≈95\%. They also report that the mode of k* (where
max\textbar Dk\textbar{} occurs) is at the true variance-change point
and concentrates there as T or the variance ratio increases.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
What makes the ICSS algorithm work well is that the mode of k* is
exactly at the point where the change in variance occurs.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=140-310, section=Theorem 1, note=Asymptotic null: Dk
    converges to a Brownian bridge (W0).
  \item
    page=5, words=0-120, section=Table 2, note=Monte Carlo: percentage
    with max\textbar Dk\textbar{} \textless{} 1.358 near 95\% (10,000
    N(0,1) replicates).
  \item
    page=9, words=360-500, section=4.4, note=Distribution of k*: mode
    exactly at the true change point; concentration increases with T and
    variance ratio.
  \end{itemize}
\end{itemize}

\subsubsection{B11: Are there event-study or pre-trend tests? What do
they
show?}\label{b11-are-there-event-study-or-pre-trend-tests-what-do-they-show}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{medium} (0.38895839421501494)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\ description}
\item
  Data source: Inclán and Tiao (1994), JASA 89(427): 913--923
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Within the provided excerpts, there is no mention of
  event-study or pre-trend tests; content centers on ICSS variance
  change detection, simulations, and an IBM stock example.
\item
  Answer:
\end{itemize}

No. The article does not report event-study or pre-trend tests; it
focuses on detecting variance changes using the ICSS algorithm and
compares it with likelihood ratio and Bayesian methods.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
We propose a procedure to detect variance changes based on an iterated
cumulative sums of squares (ICSS) algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction/scope, note=States the
    paper's focus on ICSS for variance change detection.
  \item
    page=2, words=275-624, section=Method overview, note=Describes
    cumulative sums of squares approach and paper organization; no
    mention of event-study or pre-trends.
  \item
    page=7, words=0-349, section=Simulation experiment, note=Details
    simulations comparing ICSS to likelihood ratio and Bayesian
    approaches; no event-study or pre-trend tests.
  \item
    page=6, words=275-624, section=Empirical example, note=IBM stock
    price example analyzed via ICSS; no event-study framework discussed.
  \end{itemize}
\end{itemize}

\subsubsection{B12: What instruments are used (if IV)? Define instrument
and first
stage.}\label{b12-what-instruments-are-used-if-iv-define-instrument-and-first-stage}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.1772272201535017)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Data source: Provided context excerpts from Inclán \& Tiao on ICSS and
  variance change detection
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: No IV terminology or regression framework appears in
  the provided excerpts; inferred that instruments and first-stage
  definitions are not applicable to this method-focused variance-change
  study.
\item
  Answer:
\end{itemize}

Not applicable. The provided context describes the ICSS algorithm for
detecting variance changes in time series and does not use instrumental
variables (IV). Therefore, there are no instruments or first-stage
regressions to define in this study.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Iterated Cumulative Sums of Squares (ICSS) Algorithm
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=900-1120, section=3. Detections of Multiple Changes:
    The Iterated Cumulative Sums of Squares Algorithm, note=Introduces
    and details the ICSS algorithm; no mention of IV or instruments.
  \item
    page=6, words=0-120, section=ICSS discussion and convergence,
    note=Describes algorithmic convergence and usage; context is
    variance-change diagnostics, not IV.
  \item
    page=6, words=550-880, section=Procedure refinement and example,
    note=Focuses on systematic search and diagnostics for variance
    changes; no IV framework.
  \end{itemize}
\end{itemize}

\subsubsection{B13: What is the first-stage strength (F-stat, partial
R\^{}2, relevance
evidence)?}\label{b13-what-is-the-first-stage-strength-f-stat-partial-r2-relevance-evidence}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.3120605828645241)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not available in the provided context. The text does not report any
first-stage regression results (e.g., IV first-stage F-statistic,
partial R\^{}2, or other relevance evidence). The only F-statistic
discussed pertains to testing equality of variances in variance-change
detection, not instrument relevance.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
For a fixed k, the value of Dk can be written as a function of the usual
F statistic for testing equality of variances... F\_\{T−k,k\} = ((CT −
Ck)/(T − k))/(Ck/k).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=0-349, section=2.1 Relationship of Dk to the F
    Statistic and the Likelihood Ratio, note=F-stat here is for equality
    of variances, not IV first-stage strength.
  \end{itemize}
\end{itemize}

\subsubsection{B14: If RDD: what is the running variable and cutoff?
bandwidth
choice?}\label{b14-if-rdd-what-is-the-running-variable-and-cutoff-bandwidth-choice}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{low} (0.3424343411773066)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Table/Figure: \texttt{Figure\ 4}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not applicable. The study uses ICSS/change-point detection for variance
shifts, not a regression discontinuity design; thus there is no running
variable, no cutoff, and no bandwidth choice.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
we will be looking for maxk \textbar Dk\textbar{} to determine the
location of the change point.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=0-349, section=Section 2.1, note=Method looks for
    max\textbar Dk\textbar{} to locate a change point, not an RDD setup.
  \item
    page=3, words=275-624, section=Sections 2.1--2.2, note=Focus on
    testing variance changes and using Dk/CUSUM of squares.
  \item
    page=6, words=275-624, section=Section 3.1, note=ICSS algorithm
    performs a systematic search for points of change.
  \item
    page=9, words=275-624, section=Section 4.4, note=Discussion of k*
    where max\textbar Dk\textbar{} is attained; confirms change-point
    framework.
  \end{itemize}
\end{itemize}

\subsubsection{B15: If DiD: what is the timing variation (staggered
adoption)? estimator
used?}\label{b15-if-did-what-is-the-timing-variation-staggered-adoption-estimator-used}

\begin{itemize}
\tightlist
\item
  Category: \texttt{B)\ Identification\ strategy\ /\ causal\ design}
\item
  Confidence: \texttt{medium} (0.41074100604643277)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Table/Figure: \texttt{Table\ 1}
\item
  Data source: Journal of the American Statistical Association,
  September 1994 (Inclán \& Tiao)
\item
  Assumption flag: \texttt{False}
\item
  Assumption notes: The provided context covers variance change-point
  detection (ICSS), not difference-in-differences.
\item
  Answer:
\end{itemize}

Not applicable. The study is not a DiD design; it detects variance
change points using the ICSS (Iterative Cumulative Sum of Squares)
algorithm, estimating changepoints via max\_k \textbar Dk\textbar{} with
Brownian-bridge critical values (Table 1). There is no staggered
adoption or DiD estimator.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This behavior leads to the search for a variance change point via maxk I
Dk I . Let k* be the value of k at which maxk I Dk I is attained.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=550-882, section=Method: change-point search,
    note=Defines search via max\_k \textbar Dk\textbar{} to locate
    variance change points.
  \item
    page=5, words=0-349, section=Asymptotics and boundaries, note=Dk
    -\textgreater{} Brownian bridge; Table 1 gives quantiles for
    decision thresholds.
  \item
    page=5, words=1100-1198, section=Algorithm steps, note=Uses boundary
    D* from Table 1 to declare changepoints.
  \item
    page=6, words=0-349, section=ICSS algorithm usage, note=Describes
    the ICSS algorithm and its purpose in detecting variance changes.
  \end{itemize}
\end{itemize}

\subsubsection{C01: What dataset(s) are used? (name sources
explicitly)}\label{c01-what-datasets-are-used-name-sources-explicitly}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.26690114886370353)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{direct\_quote\_and\_summary}
\item
  Data source: Box and Jenkins (1976); author-generated simulated series
  and AR(1) residuals
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The paper uses: (1) a real dataset of IBM stock closing prices (analyzed
as first differences of log prices) from May 17, 1961 to November 2,
1962, as reported by Box and Jenkins (1976); and (2) simulated time
series (and AR(1) residuals) generated by the authors for the simulation
experiments.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices ... as reported by Box and Jenkins (1976).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=275-624, section=3.1 Example: IBM Stock Prices,
    note=Describes the IBM stock prices dataset and cites Box and
    Jenkins (1976).
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=States
    that simulation experiments were conducted with generated series.
  \item
    page=9, words=550-899, section=4.5 The ICSS Algorithm Applied to
    Residuals, note=Details application to AR(1) residuals from
    simulated series.
  \end{itemize}
\end{itemize}

\subsubsection{C02: What is the unit of observation (individual,
household, firm, county, transaction,
product)?}\label{c02-what-is-the-unit-of-observation-individual-household-firm-county-transaction-product}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.24640099422268022)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Data source: IBM stock closing prices (Box and Jenkins, 1976)
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Unit inferred from the empirical example focused on
  IBM, a single firm; other parts use simulated series.
\item
  Answer:
\end{itemize}

firm

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices from May 17, 196 1, to No- vember 2, 1962, as
reported by Box and Jenkins (1976).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=275-624, section=3.1 Example: IBM Stock Prices,
    note=Empirical example analyzes IBM stock closing prices (a single
    company\textquotesingle s series).
  \end{itemize}
\end{itemize}

\subsubsection{C03: What is the sample period and geographic
coverage?}\label{c03-what-is-the-sample-period-and-geographic-coverage}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.268446744281151)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{article\_text}
\item
  Data source: Box and Jenkins (1976) IBM stock closing prices
\item
  Assumption flag: \texttt{False}
\item
  Assumption notes: Geographic coverage is not specified in the provided
  context.
\item
  Answer:
\end{itemize}

Sample period: May 17, 1961 to November 2, 1962. Geographic coverage:
Not specified in the provided context (IBM stock closing prices
dataset).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices from May 17, 1961, to November 2, 1962, as reported
by Box and Jenkins (1976).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=275-624, section=3.1 Example: IBM Stock Prices,
    note=States the IBM stock series dates: May 17, 1961 to November 2,
    1962.
  \end{itemize}
\end{itemize}

\subsubsection{C04: What are the sample restrictions / inclusion
criteria?}\label{c04-what-are-the-sample-restrictions--inclusion-criteria}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.23034433773356772)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\ description\ (methods)}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interprets \textquotesingle sample\textquotesingle{}
  as the simulated series used in the study (not an observational
  dataset).
\item
  Answer:
\end{itemize}

Simulated time-series only. Inclusion criteria: (1) innovations assumed
Normal with mean 0 and piecewise-constant variance; (2) series lengths T
in \{100, 200, 500\}; (3) one-change cases use K1 at \{0.25T, 0.50T,
0.75T\} with variance ratio A in \{2, 3\}; (4) two-change cases use (K1,
K2) in \{(.33T, .66T), (.20T, .80T), (.58T, .80T)\} with base variance
set to 1 and varying ratios A1 = σ1\^{}2/σ0\^{}2 and A2 =
σ2\^{}2/σ1\^{}2; symmetric variance permutations excluded; (5) the same
generated series are subjected to ICSS, likelihood ratio, and Bayesian
procedures for paired comparisons; (6) additional runs apply ICSS to
residuals from AR(1) models fitted to the same simulated series.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The same series is subject to each of the procedures considered to
facilitate paired comparisons between procedures.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=Defines
    T, change-point locations, variance ratios; same series used across
    procedures; variance permutations and base variance constraint.
  \item
    page=3, words=0-349, section=2.1 Relationship of Dk to the F
    Statistic and the Likelihood Ratio, note=States the Normal, mean-0
    assumption for \{a\_t\}.
  \item
    page=9, words=550-899, section=4.5 The ICSS Algorithm Applied to
    Residuals, note=ICSS applied to residuals from AR(1) fitted to the
    same series.
  \end{itemize}
\end{itemize}

\subsubsection{C05: What is the sample size (N) in the main
analysis?}\label{c05-what-is-the-sample-size-n-in-the-main-analysis}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.29216650868207844)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Table/Figure: \texttt{Figure\ 1}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interpreted \textquotesingle main
  analysis\textquotesingle{} as the primary empirical series analyzed
  around Figure 1/Section 3, where indices run k=1,...,700.
\item
  Answer:
\end{itemize}

700

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Dk plot k=1,....700
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=200-230, section=Figure 1 caption/context, note=Shows
    Dk plot indexed k=1,...,700, indicating series length 700.
  \item
    page=6, words=820-870, section=ICSS algorithm discussion,
    note=Refers to Dk(a{[}398: 700{]}), confirming T ends at 700.
  \end{itemize}
\end{itemize}

\subsubsection{C06: How is the key outcome measured? Any transformations
(logs, z-scores,
indices)?}\label{c06-how-is-the-key-outcome-measured-any-transformations-logs-z-scores-indices}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.2937680977760846)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Direct\ quotes\ and\ contextual\ summary\ from\ article\ text}
\item
  Table/Figure:
  \texttt{Table\ 1\ (Empirical\ and\ Asymptotic\ Quantiles\ of\ max\ \textbar{}Dk\textbar{})}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Answer:
\end{itemize}

The key outcome is the variance-change statistic Dk, defined as a
centered and normalized cumulative sum of squares of the series.
Evidence for a change is assessed by maxk \textbar Dk\textbar{}
exceeding asymptotic Brownian-bridge boundaries (e.g., Table 1), with
the change-point estimated at k* where the maximum occurs.
Transformations used include: (1) the intrinsic centering/normalization
in Dk; (2) applying ICSS to transformed data such as the first
difference of log prices (IBM example); and (3) applying ICSS to
residuals from an AR(1) fit to remove autocorrelation.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
the centered (and normalized) cumulative sum of squares.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=825-916, section=Definition/overview, note=Defines Dk
    as the centered and normalized cumulative sum of squares; behavior
    around 0 for homogenous variance.
  \item
    page=3, words=550-882, section=Detection rule and asymptotics,
    note=Decision via max \textbar Dk\textbar{} vs boundary;
    Brownian-bridge behavior; Table 1 quantiles.
  \item
    page=6, words=275-624, section=Example (IBM), note=Uses first
    difference of the logarithm of IBM stock prices.
  \item
    page=9, words=550-899, section=Residuals application, note=Applies
    ICSS to residuals from an AR(1) model.
  \end{itemize}
\end{itemize}

\subsubsection{C07: How is treatment/exposure measured? Any constructed
variables?}\label{c07-how-is-treatmentexposure-measured-any-constructed-variables}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.2466897861561292)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\_evidence}
\item
  Data source: Methodological simulations with independent N(0,1)
  series; no treatment/exposure variables
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not applicable. This is a methodological/statistical paper; no treatment
or exposure is measured. The study analyzes sequences of independent
observations (often simulated N(0,1) series) to detect variance changes.
Constructed variables/statistics include the centered cumulative sum of
squares Dk = Ck/Cn − k/n (with Ck the cumulative sum of squares), its
linkage to the F statistic, and likelihood ratio statistics standardized
to an extreme value distribution within the ICSS framework.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Let Dk = Ck/Cn - k/n, where Ck = Cfi=la:.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=States focus on
    detecting multiple changes of variance in a sequence of independent
    observations; no treatment/exposure construct.
  \item
    page=5, words=0-349, section=Theorem 1 and definitions, note=Defines
    Dk = Ck/Cn − k/n with Ck as cumulative sum of squares; also notes
    simulations with independent N(0,1) observations.
  \item
    page=3, words=0-349, section=Relationship of Dk to the F Statistic,
    note=Explains Dk as a function of the usual F statistic, reinforcing
    that constructed statistics are used.
  \item
    page=8, words=0-329, section=Likelihood ratio standardization,
    note=Describes standardizing LR statistics to an extreme value
    distribution; another constructed diagnostic.
  \end{itemize}
\end{itemize}

\subsubsection{C08: Are there key covariates/controls? Which ones are
always
included?}\label{c08-are-there-key-covariatescontrols-which-ones-are-always-included}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.2842639867015846)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

No. The method analyzes sequences of observations (or residuals) to
detect variance shifts and does not include covariates or control
variables. None are always included.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This article studies the problem of multiple change points in the
variance of a sequence of independent observations.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=States the focus on
    variance changes in a sequence of independent observations, not on
    covariates.
  \item
    page=2, words=550-899, section=2. Centered Cumulative Sums of
    Squares, note=Defines the setup using uncorrelated mean-zero
    variables and cumulative sums of squares; no covariates.
  \item
    page=3, words=275-624, section=Methods/Likelihood formulation,
    note=Assumes \{a\_t\} Normally distributed with mean 0 and
    time-varying variances; focuses on variance change testing.
  \item
    page=10, words=0-84, section=Results (tables), note=Mentions
    application to AR(1) residuals, reinforcing that inputs are residual
    series, not covariates.
  \end{itemize}
\end{itemize}

\subsubsection{C09: How are missing data handled (dropping, imputation,
weighting)?}\label{c09-how-are-missing-data-handled-dropping-imputation-weighting}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.24359517343422313)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{not\ specified\ in\ context}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association (provided context)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not discussed in the provided context; there is no description of
handling missing data (e.g., dropping, imputation, or weighting).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
It is important to look at the plots, because a big outlier would create
a significant peak in the Dk plot... advisable to complement the search
with a procedure for outlier detection.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=0-349, section=ICSS algorithm discussion / 3.1
    Example: IBM Stock Prices, note=Context discusses outliers and ICSS
    procedure but gives no guidance on missing data handling.
  \end{itemize}
\end{itemize}

\subsubsection{C10: Are weights used (survey weights, propensity
weights)?
How?}\label{c10-are-weights-used-survey-weights-propensity-weights-how}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.28048630291205834)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Inference\ from\ absence\ in\ provided\ methodological\ description}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association: Use of Cumulative Sums of Squares for
  Retrospective Detection of Changes of Variance
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Weights are not mentioned in the provided excerpts;
  conclusion is based on the absence of any reference to
  survey/propensity weighting and the focus on unweighted cumulative
  sums of squares.
\item
  Answer:
\end{itemize}

No. The methods and simulations described rely on unweighted cumulative
sums of squares (the ICSS algorithm) and related likelihood/Bayesian
comparisons; survey or propensity weights are not mentioned or used.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
We propose a procedure to detect variance changes based on an iterated
cumulative sums of squares (ICSS) algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=Describes ICSS
    approach; no mention of any weighting scheme.
  \item
    page=2, words=825-916, section=Introduction/Method, note=Defines
    centered/normalized cumulative sum of squares Dk; no weights
    referenced.
  \item
    page=7, words=0-349, section=Simulation experiment setup,
    note=Details simulation design and procedures compared; no use of
    survey or propensity weights.
  \end{itemize}
\end{itemize}

\subsubsection{C11: Are data linked/merged across sources? How is
linkage
performed?}\label{c11-are-data-linkedmerged-across-sources-how-is-linkage-performed}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.20254398734717888)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{direct\_text}
\item
  Table/Figure: \texttt{Table\ 1\ (note)}
\item
  Data source: IBM stock prices (Box and Jenkins, 1976); simulated
  N(0,1) series
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

No. The analyses use either a single time series (IBM stock prices from
Box and Jenkins, 1976) or simulated independent N(0,1) series. There is
no cross-source linkage or merging performed.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=275-624, section=3.1 Example: IBM Stock Prices,
    note=Describes use of a single real-world series (IBM stock prices)
    from a cited source.
  \item
    page=3, words=550-882, section=Table 1 note, note=States results are
    from simulations of independent N(0,1) series.
  \end{itemize}
\end{itemize}

\subsubsection{C12: What summary statistics are reported for main
variables?}\label{c12-what-summary-statistics-are-reported-for-main-variables}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{medium} (0.3529576886460369)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Tables\ and\ accompanying\ text\ in\ the\ provided\ excerpts}
\item
  Table/Figure: \texttt{Table\ 1;\ Table\ 2;\ Table\ 7;\ Table\ 8}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interpreted the \textquotesingle main
  variables\textquotesingle{} as k*, max\textbar Dk\textbar, and ICSS
  detection proportions (NT=1). If a different set of variables was
  intended, the reported summaries may differ.
\item
  Answer:
\end{itemize}

They report: (1) for k*: mode, mean, and standard deviation; (2) for
max\textbar Dk\textbar: empirical and asymptotic quantiles with standard
errors; and (3) detection rates as percentages of series meeting
criteria (e.g., max\textbar Dk\textbar{} \textless{} 1.358) with
standard errors, plus percentages of NT = 1 from ICSS runs.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Mode of k* Mean of k* SD of k*
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=550-882, section=Table 1, note=Empirical and
    asymptotic quantiles of max\textbar Dk\textbar{} with SEs are
    presented.
  \item
    page=5, words=0-349, section=Table 2, note=Percentages of series
    with max\textbar Dk\textbar{} \textless{} 1.358 and their standard
    errors.
  \item
    page=10, words=0-84, section=Table 7 and Table 8, note=Summaries for
    k* (Mode, Mean, SD) and percentage of NT = 1.
  \end{itemize}
\end{itemize}

\subsubsection{C13: Are there descriptive figures/maps that establish
baseline
patterns?}\label{c13-are-there-descriptive-figuresmaps-that-establish-baseline-patterns}

\begin{itemize}
\tightlist
\item
  Category: \texttt{C)\ Data,\ sample,\ and\ measurement}
\item
  Confidence: \texttt{low} (0.289297076290918)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\_figure\_reference}
\item
  Table/Figure: \texttt{Figure\ 1;\ Figure\ 2;\ Figure\ 3;\ Figure\ 4}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. The paper includes descriptive figures (e.g., Figure 1) that show
baseline patterns under homogeneous variance and illustrate expected Ck
and Dk behaviors, with additional figures for change-point scenarios.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
For the series with homogeneous variance, Ck presents roughly a straight
line with slope a2 = 1.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=275-624, section=Figure 1 discussion, note=Mentions
    Figure 1 and example series used to illustrate baseline and
    changed-variance patterns.
  \item
    page=3, words=550-882, section=Main text on baseline behavior,
    note=Describes baseline pattern for homogeneous variance (straight
    line for Ck, characteristic Dk behavior).
  \item
    page=4, words=550-869, section=Figure 3 caption/discussion,
    note=References Figure 3 showing expected Dk patterns with specified
    change points.
  \item
    page=6, words=275-624, section=Figure 4 caption/discussion,
    note=References Figure 4 sequence of Dk functions, further
    illustrating patterns.
  \end{itemize}
\end{itemize}

\subsubsection{D01: What is the headline main effect estimate (sign and
magnitude)?}\label{d01-what-is-the-headline-main-effect-estimate-sign-and-magnitude}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.276356738767391)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Answer:
\end{itemize}

Not reported in the provided context; no main-effect estimate (sign or
magnitude) is given.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Note that although LRo,, and max, \textbar Dk\textbar{} are related,
they are not equivalent for finite T.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=0-349, section=2.1, note=Context discusses Dk,
    F-statistic relation, and critical value, but no main-effect
    coefficient is presented.
  \item
    page=4, words=275-624, section=2.3, note=Explains properties of
    max\textbar Dk\textbar{} and likelihood ratio but does not report
    any main-effect estimate.
  \end{itemize}
\end{itemize}

\subsubsection{D02: What is the preferred specification and why is it
preferred?}\label{d02-what-is-the-preferred-specification-and-why-is-it-preferred}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.22501087583053234)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interpreted `preferred specification' as the
  preferred method/approach for variance-change detection in this
  context; the article recommends ICSS for long series with multiple
  changepoints.
\item
  Answer:
\end{itemize}

Use the ICSS algorithm for detecting variance changepoints---especially
in long series with multiple changes---because it has O(T) computational
cost and, in simulations, performs as well or better than likelihood
ratio tests as T grows, while Bayesian posterior odds are
computationally burdensome.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
\ldots lend support to the recommendation to use the ICSS algorithm when
we need to analyze long series with multiple change points.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=11, words=0-349, section=4.6 CPU Time Requirements,
    note=Explicit recommendation to use ICSS for long series with
    multiple change points
  \item
    page=11, words=275-507, section=4.6 CPU Time Requirements, note=CPU
    time order: LR roughly O(T\^{}2) for NT≥2; ICSS dependence
    consistent across NT
  \item
    page=9, words=0-349, section=Results of Part Two (Two Changepoints),
    note=ICSS often better than likelihood ratio tests with ≥200
    observations
  \item
    page=9, words=550-899, section=4.6 CPU Time Requirements (lead-in),
    note=Heavy computation for LR/Bayesian; ICSS avoids intensive
    function evaluations
  \end{itemize}
\end{itemize}

\subsubsection{D03: How economically meaningful is the effect (percent
change, elasticity,
dollars)?}\label{d03-how-economically-meaningful-is-the-effect-percent-change-elasticity-dollars}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.30583567056319455)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not specified. The context describes a statistical method (ICSS) for
detecting variance change points and reports thresholds/quantiles, but
it does not quantify an economic effect in percent change, elasticity,
or dollars.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This behavior leads to the search for a variance change point via maxk I
Dk I.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=560-700, section=2. CENTERED CUMULATIVE SUMS OF
    SQUARES, note=Describes searching for variance change points via
    max\textbar Dk\textbar; no economic magnitude reported.
  \item
    page=6, words=275-420, section=3.1 Example: IBM Stock Prices,
    note=Example uses stock returns and reports change points only, not
    economic effect sizes.
  \item
    page=5, words=0-120, section=Asymptotic behavior/quantiles,
    note=Presents asymptotic results and percentages for
    max\textbar Dk\textbar; still methodological, not economic
    magnitudes.
  \end{itemize}
\end{itemize}

\subsubsection{D04: What are the key robustness checks and do results
survive
them?}\label{d04-what-are-the-key-robustness-checks-and-do-results-survive-them}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.33810902789115976)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{Simulation\ and\ empirical\ corroboration}
\item
  Table/Figure: \texttt{Table\ 2;\ Table\ 4;\ Figure\ 4}
\item
  Data source: Simulated N(0,1) series; IBM stock prices (1961--1962)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Key checks: (1) Monte Carlo simulations varying series length,
changepoint location, and variance ratios for both one and two
changepoints; results show correct identification improves with longer T
and larger variance ratios, exceeding 80\% for T≥200 or A=3 when the
change is central, while A=2 with T=100 and early change is harder. (2)
Applying ICSS to AR(1) residuals yields detection rates very close to
the baseline simulation, indicating robustness to modeling residuals.
(3) Real-data corroboration: on IBM returns, ICSS changepoints match
maximum-likelihood estimates and Bayesian posterior modes at the same
points. (4) Type I error check: empirical 95th quantile of
max\textbar Dk\textbar{} is below the asymptotic 1.358, implying
conservative size under the null. (5) Localization property: the mode of
k* is at the true changepoint and concentrates with larger T or variance
ratio. Overall, the main results survive these robustness checks.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
the percentage of series for which NT = 1 is very close to the figures
presented in Table 4.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=Simulation design (T, K/T, A; one and
    two changepoints), note=Design spans T=100,200,500; K/T=.25,.50,.75;
    A=2,3; and multiple two-changepoint configurations.
  \item
    page=9, words=550-899, section=Simulation results summary,
    note=Performance patterns: harder at T=100, A=2 (early change);
    \textgreater80\% when T≥200 or A=3 in middle; improvement with T and
    A.
  \item
    page=9, words=550-899, section=ICSS applied to AR(1) residuals,
    note=Residuals check shows NT=1 rates very close to Table 4.
  \item
    page=6, words=550-899, section=IBM example; cross-method
    corroboration, note=ICSS points agree with ML; Bayesian posterior
    modes at same points.
  \item
    page=5, words=275-624, section=Type I error/quantiles,
    note=Empirical 95th quantile \textless{} asymptotic 1.358, implying
    smaller empirical type I error.
  \item
    page=9, words=275-624, section=Distribution of k*, note=Mode at true
    change; concentration increases with T and variance ratio.
  \end{itemize}
\end{itemize}

\subsubsection{D05: What placebo tests are run and what do they
show?}\label{d05-what-placebo-tests-are-run-and-what-do-they-show}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.20050309178168896)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{simulation\ (Table\ 2)\ and\ asymptotic\ theory}
\item
  Table/Figure: \texttt{Table\ 2}
\item
  Data source: 10,000 simulated IID N(0,1) series per T under no change
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Placebo (null) tests simulate IID N(0,1) series with no variance changes
and compute max\_k \textbar Dk\textbar{} against the asymptotic boundary
1.358. Across T=100,200,300,400,500 (10,000 reps each), 97.13\%,
96.51\%, 96.31\%, 96.07\%, and 95.53\% of series stay below 1.358,
showing false positives are near the intended \textasciitilde5\% and
consistent with Brownian-bridge asymptotics.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
NOTE: 10,000 replicates, series of T independent N ( 0 , 1 )
observations.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=0-349, section=Table 2: Null simulation results,
    note=Percent of series with max\textbar Dk\textbar{} \textless{}
    1.358 for T=100--500; 10,000 replicates of IID N(0,1)
  \item
    page=5, words=0-349, section=Theorem 1, note=Asymptotic result: Dk
    converges to a Brownian bridge, basis for boundary 1.358
  \item
    page=3, words=0-349, section=Figure discussion / critical value,
    note=Asymptotic critical value D.35 = 1.358 and boundary
    interpretation for \textbar Dk\textbar{}
  \end{itemize}
\end{itemize}

\subsubsection{D06: What falsification outcomes are tested (unaffected
outcomes)?}\label{d06-what-falsification-outcomes-are-tested-unaffected-outcomes}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.28755518983303757)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Data source: Inclán and Tiao (1994), JASA, provided excerpts
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Not specified in the provided context; no falsification (unaffected)
outcomes are discussed or tested.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The simulation experiment has two separate parts, the first for one
changepoint and the second for two changepoints
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=300-460, section=4. SIMULATION EXPERIMENT,
    note=Context focuses on simulation to detect variance changes; no
    mention of falsification or unaffected outcomes.
  \end{itemize}
\end{itemize}

\subsubsection{D07: What heterogeneity results are reported (by income,
size, baseline exposure,
region)?}\label{d07-what-heterogeneity-results-are-reported-by-income-size-baseline-exposure-region}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.32051980795754204)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

None. The provided context reports no heterogeneity results by income,
size, baseline exposure, or region.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The simulation experiment has two separate parts, the first for one
changepoint and the second for two changepoints
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=275-556, section=4. Simulation experiment setup and
    focus, note=Describes simulations focused on detecting variance
    changepoints, not subgroup heterogeneity.
  \item
    page=3, words=550-882, section=Method description and Table 1
    context, note=Focus on variance change detection via Dk and
    empirical/asymptotic quantiles.
  \item
    page=9, words=275-624, section=Posterior odds and sampling
    distribution of k*, note=Discusses number of variance changes and
    distribution of the estimated changepoint.
  \end{itemize}
\end{itemize}

\subsubsection{D08: What mechanism tests are performed and what do they
imply?}\label{d08-what-mechanism-tests-are-performed-and-what-do-they-imply}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.2055530935249787)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\_evidence\_and\_simulation\_results}
\item
  Data source: Inclán and Tiao (1994), JASA -- provided excerpts
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

They perform three mechanisms to test for variance changes: (1) the ICSS
max\textbar Dk\textbar{} threshold test applied iteratively to segments
(if max\textbar Dk\textbar{} ≤ D*, no change; if \textgreater{} D*,
declare a changepoint and continue), (2) sequential likelihood-ratio
tests LR\_\{n−1,n\} to assess the number of changepoints, standardized
to an extreme-value reference distribution, and (3) Bayesian
posterior-odds K\_\{n,n−1\} to select the number of changepoints up to a
specified maximum. Implications: max\textbar Dk\textbar{} is biased
toward mid-series but its mode occurs at the true changepoint and
concentrates there as T or the variance ratio increases; small changes
(A=2) in short series (T=100), especially early in the series, are hard
to detect with ICSS, while for T≥200 or A=3, correct identification
exceeds \textasciitilde80\% when the change is near the middle; the
Bayesian approach performs best for A=2, and ICSS improves with larger T
and A. They also note F-test power features for variance differences.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
If M(t1:T) \textless{} D*, there is no evidence of variance changes...
If M(t1:t2) \textgreater{} D*, then we have a new point of change
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=0-349, section=ICSS algorithm steps, note=Decision
    rule for ICSS: compare max\textbar Dk\textbar{} to D*; if below,
    stop; if above, declare a change and iterate.
  \item
    page=8, words=0-329, section=4.1 Likelihood ratios and posterior
    odds, note=LR\_\{n−1,n\} tests, standardization to extreme value
    distribution; posterior odds framework.
  \item
    page=7, words=275-556, section=4.1 Likelihood ratios and posterior
    odds, note=Definition/use of LR\_\{n−1,n\} for testing numbers of
    changepoints.
  \item
    page=9, words=275-624, section=Distribution of k* and implications,
    note=k* is biased toward middle; mode at true changepoint;
    concentration increases with T or variance ratio.
  \item
    page=9, words=550-899, section=4.2 Results (one changepoint),
    note=Performance implications: hard to detect A=2 with T=100; ≥80\%
    correct for T≥200 or A=3 when change is in the middle; Bayesian best
    for A=2; ICSS improves with T and A.
  \item
    page=4, words=0-349, section=Discussion of tests, note=F-test power
    characteristics for variance differences and interaction with
    degrees of freedom.
  \end{itemize}
\end{itemize}

\subsubsection{D09: How sensitive are results to alternative
samples/bandwidths/controls?}\label{d09-how-sensitive-are-results-to-alternative-samplesbandwidthscontrols}

\begin{itemize}
\tightlist
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{medium} (0.4155334148582157)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{simulation\ results\ and\ asymptotic\ theory\ from\ the\ provided\ text}
\item
  Table/Figure: \texttt{Tables\ 2,\ 4,\ 6–7\ (as\ referenced\ in\ text)}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: No bandwidths or control-variable choices are
  discussed; sensitivity is interpreted in terms of sample size,
  variance ratio, change-point location, and configuration as reported.
\item
  Answer:
\end{itemize}

Sensitivity is driven by sample length (T), variance ratio (A), and
where/how variance changes occur---bandwidths/controls are not
discussed. Specifically: (1) With T=100 and small A=2, detection is
hard, especially for early changes; with T≥200 or A=3 and a mid-series
change, correct identifications exceed \textasciitilde80\%, and
performance improves with larger T and A. (2) With two changepoints,
monotone variance increases are most difficult; T≈500 is often needed to
get NT=2 more than half the time; best results occur when changes are
equally spaced and the largest variance is in the middle. (3)
Finite-sample critical values are conservative: the 95th quantile of
max\textbar Dk\textbar{} is below the asymptotic 1.358, lowering type I
error. (4) The location estimator k* is skewed toward the middle but has
its mode at the true change and concentrates there as T or A increase.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Once we have 200 observations or the variance ratio is larger (A = 3),
the correct identifications occur more than 80\% of the time
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=9, words=550-899, section=Simulation results (one changepoint),
    note=Detection depends on T, A, and position; \textgreater80\% with
    T≥200 or A=3 when change is in the middle; harder with T=100 and
    A=2, especially early changes.
  \item
    page=9, words=0-349, section=Simulation results (two changepoints),
    note=Monotone increases are hardest; need large T (e.g., 500) to get
    NT=2 more than half the time; best when changes are equally spaced
    and large variance is in the middle.
  \item
    page=9, words=275-624, section=Sampling distribution of k*, note=k*
    is biased toward the center but its mode is at the true change;
    concentration improves with larger T or A.
  \item
    page=5, words=275-624, section=Finite-sample quantiles, note=95th
    quantile of max\textbar Dk\textbar{} is below asymptotic 1.358,
    implying smaller empirical type I error (conservative critical
    value).
  \end{itemize}
\end{itemize}

\subsubsection{D10: What are the main takeaways in the conclusion
(bullet
summary)?}\label{d10-what-are-the-main-takeaways-in-the-conclusion-bullet-summary}

\begin{itemize}
\item
  Category:
  \texttt{D)\ Results,\ magnitudes,\ heterogeneity,\ robustness}
\item
  Confidence: \texttt{low} (0.204945056307743)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{Paraphrase\ with\ direct\ citations}
\item
  Table/Figure: \texttt{Tables\ 4–8}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: The explicit conclusion bullets were not provided;
  key takeaways are summarized from the provided conclusion and results
  excerpts.
\item
  Answer:
\item
  ICSS is recommended for analyzing long series with multiple variance
  change points, based on results and CPU-time evidence.
\item
  Computationally efficient: iterative search averages O(T) operations
  and avoids exhaustive evaluation; convergence typically in few
  iterations.
\item
  One change point: with T=100 and small change (A=2), detection is
  hard---especially early in the series; with T≥200 or larger change
  (A=3) near the middle, correct identification exceeds
  \textasciitilde80\%. Bayesian is best for A=2 and comparable for A=3.
\item
  Two change points: with T≥200, ICSS generally outperforms likelihood
  ratio tests; best when change points are evenly spaced (.33T, .67T)
  and when the largest variance is in the middle (A1,A2)=(4,2).
  Monotone-increasing variances require larger T (e.g., 500) to identify
  both changes \textgreater50\% of the time.
\item
  Posterior-odds approach is computationally prohibitive (e.g.,
  \textasciitilde1,358 CPU hours for T=500), limiting practicality.
\item
  Location statistic k*: distribution is skewed and biased toward the
  middle, but the mode aligns with the true change; concentration around
  the true point improves with larger T and variance ratios.
\item
  Applying ICSS to AR(1) residuals yields similar identification rates
  to the i.i.d. case.
\item
  Quote snippet:
\end{itemize}

\begin{quote}
\ldots lend support to the recommendation to use the ICSS algorithm when
we need to analyze long series with multiple change points.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=11, words=0-349, section=CPU Time Requirements / Conclusion,
    note=Recommends ICSS for long series with multiple change points;
    O(T) operations.
  \item
    page=5, words=825-1174, section=ICSS Algorithm and Convergence,
    note=Iterative scheme; convergence in few iterations.
  \item
    page=9, words=550-899, section=4.2 Results Part One; 4.6 CPU Time
    (setup), note=One change point detection difficulty at T=100, A=2;
    improves with T≥200 or A=3; Bayesian best at A=2.
  \item
    page=9, words=0-349, section=4.3 Results Part Two, note=ICSS better
    than LR with T≥200; best at (.33,.67) and when large variance in the
    middle; posterior odds computational burden (1,358 CPU hours).
  \item
    page=9, words=275-624, section=4.4 Sampling Distribution of k*,
    note=k* mode at true change; distribution biased to middle;
    concentration improves with T and variance ratio.
  \item
    page=9, words=550-899, section=4.5 ICSS on Residuals, note=Similar
    identification rates on AR(1) residuals.
  \end{itemize}
\end{itemize}

\subsubsection{E01: What are the most important prior papers cited and
why are they central
here?}\label{e01-what-are-the-most-important-prior-papers-cited-and-why-are-they-central-here}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.27717837898716885)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\ citations\ from\ the\ article}
\item
  Data source: Inclán \& Tiao (1994), JASA 89(427):913--923 (provided
  context)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Key prior works are: (1) Brown, Durbin, and Evans (1975) because the
ICSS procedure is built on their centered cumulative sum of squares; (2)
Hsu, Miller, and Wichern (1974) for initiating the variance-change
literature with a step-change variance model for stock returns; (3) Hsu
(1977, 1979, 1982) and Booth \& Smith (1982) for single-change detection
methods (frequentist and Bayesian) that highlight the computational
burden of multiple changes and serve as baselines; (4) Baufays \& Rasson
(1985) for MLE methods handling several change points and for providing
the algorithm used to compute LR benchmarks; (5) Wichern, Miller, \& Hsu
(1976) and Abraham \& Wei (1984) for variance-change models in
autocorrelated (AR) series, relevant to applying ICSS to residuals; (6)
Worsley (1986) as a related MLE framework for change-point estimation
(mean changes) that informs single-change inference.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
It is based on a centered version of the cumulative sum of squares
presented by Brown, Durbin, and Evans (1975).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=275-624, section=Introduction, note=ICSS is based on
    the centered CUSUM of squares of Brown, Durbin, and Evans (1975);
    foundational change-of-variance literature and single-change
    Bayesian/frequentist works cited.
  \item
    page=2, words=0-349, section=Introduction, note=Hsu, Miller, and
    Wichern (1974) noted as starting the variance-change literature with
    step changes in variance for stock returns.
  \item
    page=2, words=550-899, section=2. Centered Cumulative Sums of
    Squares, note=Worsley (1986) MLE for change in mean; Wichern,
    Miller, \& Hsu (1976) and Abraham \& Wei (1984) on AR(1) with
    variance change; Baufays \& Rasson (1985) MLE for multiple change
    points.
  \item
    page=9, words=550-899, section=4.6 CPU Time Requirements,
    note=Baufays \& Rasson (1985) algorithm used to obtain the ML in LR
    tests; computational motivation for ICSS.
  \end{itemize}
\end{itemize}

\subsubsection{E02: Which papers does this work most directly build on
or
extend?}\label{e02-which-papers-does-this-work-most-directly-build-on-or-extend}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.25288654187197895)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{explicit\ statements\ in\ the\ text}
\item
  Data source: Inclán \& Tiao (1994) article text (JSTOR excerpt)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The paper builds most directly on Brown, Durbin, and Evans (1975) for
the centered cumulative sum of squares, and extends the variance
change-point literature initiated by Hsu, Miller, and Wichern (1974) and
developed by Hsu (1977, 1979, 1982) and Booth \& Smith (1982).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
It is based on a centered version of the cumulative sum of squares
presented by Brown, Durbin, and Evans (1975).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=275-624, section=Introduction/Overview, note=States
    the method is based on a centered version of CUSUM of squares by
    Brown, Durbin, and Evans (1975); lists prior variance change-point
    works including Hsu (1977, 1979, 1982) and Booth \& Smith (1982).
  \item
    page=2, words=0-349, section=Introduction, note=Notes the
    statistical literature on changes of variance started with Hsu,
    Miller, and Wichern (1974).
  \end{itemize}
\end{itemize}

\subsubsection{E03: Which papers are used as benchmarks or comparisons
in the
results?}\label{e03-which-papers-are-used-as-benchmarks-or-comparisons-in-the-results}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.30012011333351307)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{textual\ evidence\ from\ methods/results\ description\ and\ cited\ references}
\item
  Data source: JASA 1994 article text (provided context)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The results compare ICSS against (1) likelihood ratio tests implemented
using the Baufays and Rasson (1985) algorithm, and (2) a Bayesian
posterior-odds approach specified with an inverted-gamma prior as in
Inclán (1991).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
compare the performance of the ICSS algorithm with the likelihood ratio
and a Bayesian approach.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=States
    comparison of ICSS with likelihood ratio and a Bayesian approach.
  \item
    page=9, words=550-899, section=4.6 CPU Time Requirements, note=LR
    tests computed using the Baufays and Rasson (1985) algorithm.
  \item
    page=9, words=275-624, section=4.4 Sampling Distribution of k* / 4.2
    Results context, note=Bayesian posterior odds uses inverted-gamma
    prior (see Inclán 1991).
  \end{itemize}
\end{itemize}

\subsubsection{E04: What data sources or datasets are cited and how are
they
used?}\label{e04-what-data-sources-or-datasets-are-cited-and-how-are-they-used}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.2478824092067886)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Table/Figure:
  \texttt{Tables\ 6–8\ (simulation\ results\ and\ AR(1)\ residuals)}
\item
  Data source: Simulated time series (T=100,200,500) with specified
  changepoints and variance ratios; AR(1) residuals derived from these
  simulations
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Only simulated series are cited. The study uses artificially generated
time series with specified lengths (T = 100, 200, 500), changepoint
locations, and variance ratios (A = 2, 3) for simulation experiments; it
also applies the ICSS algorithm to AR(1) residuals derived from these
simulated series. No external empirical datasets are cited in the
provided excerpt; an illustrative series is mentioned but not
identified.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
For Part One, we used three series lengths (T = 100, 200, and
500)\ldots{} and two values for the variance ratio (A = 2 and 3)\ldots{}
Table 8\ldots{} Residuals from AR(1).
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=Details
    of simulated series: lengths, changepoint locations, variance
    ratios; paired comparisons.
  \item
    page=9, words=0-349, section=4.4 Sampling Distribution of k*,
    note=States there are 1,000 replicates per configuration, confirming
    simulation-based data.
  \item
    page=10, words=0-84, section=Tables 7--8, note=Mentions applying
    ICSS to AR(1) residuals from the simulated series.
  \item
    page=6, words=550-899, section=Application example, note=Refers to
    an analyzed series illustrating ICSS, but the dataset is not
    identified in the excerpt.
  \end{itemize}
\end{itemize}

\subsubsection{E05: What methodological or econometric references are
cited (e.g., DiD, IV, RDD
methods)?}\label{e05-what-methodological-or-econometric-references-are-cited-eg-did-iv-rdd-methods}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.3465047268726521)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{explicit\ references\ list}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association (provided context)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The article cites methodological/econometric works on change-point
detection, CUSUM/CUSUM-of-squares, likelihood-ratio and Bayesian
methods, time-series models with changing variance/parameters, and
related asymptotics/simulation, including: Hinkley (1971); Brown,
Durbin, and Evans (1975); Smith (1975, 1980); Booth and Smith (1982,
Journal of Econometrics); Menzefricke (1981); Hsu (1977, 1979, 1982);
Haccou and Meelis (1988); Haccou, Meelis, and van de Geer (1988);
Worsley (1986); Wichern, Miller, and Hsu (1976); Abraham and Wei (1984);
Baufays and Rasson (1985); Tyssedal and Tjostheim (1988); Tsay (1988);
Chang, Tiao, and Chen (1988); Cobb (1978); Commenges, Seal, and Pinatel
(1986); Billingsley (1968); Box and Jenkins (1976); Bratley, Fox, and
Schrage (1987); Broemeling and Tsurumi (1987). No DiD, IV, or RDD
methods are cited.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Booth \& Smith (1982), \textquotesingle A Bayesian Approach to
Retrospective Identification of Change-Points\textquotesingle; Brown,
Durbin \& Evans (1975), \textquotesingle Techniques for Testing the
Constancy of Regression Relationships Over Time\textquotesingle{}
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=12, words=0-349, section=References and Appendix, note=Multiple
    methodological references listed (Booth \& Smith 1982; Brown, Durbin
    \& Evans 1975; Billingsley 1968; Box \& Jenkins 1976; etc.)
  \item
    page=12, words=275-624, section=References and Appendix,
    note=Additional change-point and likelihood-ratio references (Haccou
    \& Meelis 1988; Worsley 1986; Hsu 1977, 1979, 1982).
  \item
    page=12, words=550-858, section=References and Appendix,
    note=Further references on change-point tests and asymptotics
    (Menzefricke 1981; Smith 1975, 1980; Worsley 1986).
  \end{itemize}
\end{itemize}

\subsubsection{E06: Are there any seminal or classic references the
paper positions itself
against?}\label{e06-are-there-any-seminal-or-classic-references-the-paper-positions-itself-against}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.23712234626341297)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Data source: Inclan \& Tiao (1994), JASA (JSTOR excerpt)
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interpreted \textquotesingle positions itself
  against\textquotesingle{} as contrasting with prior benchmark methods
  and frameworks rather than refuting them.
\item
  Answer:
\end{itemize}

Yes. The paper contrasts its ICSS method with established Bayesian and
likelihood‑ratio change‑point procedures and related maximum‑likelihood
approaches, citing classic works such as Smith (1975, 1980), Booth \&
Smith (1982), Hsu (1977, 1979, 1982), Worsley (1986), and Baufays \&
Rasson (1985). It also builds on the classic CUSUM-of-squares work of
Brown, Durbin \& Evans (1975) and references the foundational
variance-change formulation of Hsu, Miller \& Wichern (1974).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
the ICSS algorithm offers results comparable to those obtained by a
Bayesian approach or by likelihood ratio tests, without the heavy
computational burden
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=Positions ICSS
    against Bayesian and likelihood ratio tests due to computational
    burden.
  \item
    page=2, words=275-624, section=Introduction, note=States approach is
    based on centered CUSUM of squares by Brown, Durbin, and Evans
    (1975); lists classic change-point references (Hinkley 1971; Smith
    1975, 1980; Booth \& Smith 1982; Hsu 1977, 1979, 1982).
  \item
    page=2, words=550-899, section=Section 2 setup, note=Mentions
    Worsley (1986) maximum-likelihood tests and AR variance-change works
    (Wichern et al. 1976; Abraham \& Wei 1984; Baufays \& Rasson 1985).
  \end{itemize}
\end{itemize}

\subsubsection{E07: Are there citations to code, data repositories, or
appendices that are essential to the
claims?}\label{e07-are-there-citations-to-code-data-repositories-or-appendices-that-are-essential-to-the-claims}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.24801479150047653)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{explicit\ textual\ mention}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. The paper references appendices (Appendix A with derivations and
Appendix B with a proof) that underpin core claims, and it notes Fortran
code for the ICSS algorithm is available upon request. No data
repositories are cited.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The Fortran program of the ICSS algorithm is available from us upon
request; send an electronic mail message to
\href{mailto:inclan@guvax.georgetown.edu}{\nolinkurl{inclan@guvax.georgetown.edu}}.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=825-968, section=Main text, note=Mentions availability
    of Fortran program for the ICSS algorithm.
  \item
    page=11, words=0-349, section=Appendix A, note=Appendix A provides
    derivations (Approximate Expected Value of Dk).
  \item
    page=12, words=0-349, section=Appendix B, note=Appendix B provides
    the proof of Theorem 1.
  \end{itemize}
\end{itemize}

\subsubsection{E08: What gaps in the literature do the authors say these
citations leave
open?}\label{e08-what-gaps-in-the-literature-do-the-authors-say-these-citations-leave-open}

\begin{itemize}
\tightlist
\item
  Category: \texttt{E)\ Citations\ and\ related\ literature}
\item
  Confidence: \texttt{low} (0.23077763319438027)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

The cited literature mainly treats a single, unknown variance change and
tests changepoints one at a time due to heavy computational burden,
leaving a gap for an efficient, systematic method to detect multiple
variance changes in moderate-length series.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
single change of variance at an unknown point... focusing on the
detection of points of change one at a time because of the heavy
computational burden involved in looking for several points of change
simultaneously.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=275-624, section=Introduction, note=Prior work focuses
    on single changepoint; multiple points are computationally
    burdensome.
  \item
    page=2, words=0-349, section=Introduction, note=Authors position
    ICSS as comparable to Bayesian/LR without heavy computational
    burden.
  \item
    page=9, words=550-899, section=Simulation: CPU Time Requirements,
    note=Motivation cites heavy computational burden of posterior odds
    and likelihood-based searches.
  \end{itemize}
\end{itemize}

\subsubsection{F01: Are replication files or code provided? If so,
where?}\label{f01-are-replication-files-or-code-provided-if-so-where}

\begin{itemize}
\tightlist
\item
  Category: \texttt{F)\ Replication\ and\ transparency}
\item
  Confidence: \texttt{low} (0.19380720232206966)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Data source: Article text (Inclán \& Tiao, 1994) in provided context
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. The Fortran program (ICSS algorithm) is available from the authors
upon request via email to
\href{mailto:inclan@guvax.georgetown.edu}{\nolinkurl{inclan@guvax.georgetown.edu}}.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The Fortran program of the ICSS algorithm is available from us upon
request; send an electronic mail message to
\href{mailto:inclan@guvax.georgetown.edu}{\nolinkurl{inclan@guvax.georgetown.edu}}.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=825-968, note=States the Fortran program is available
    upon request via email to
    \href{mailto:inclan@guvax.georgetown.edu}{\nolinkurl{inclan@guvax.georgetown.edu}}.
  \end{itemize}
\end{itemize}

\subsubsection{F02: Is there a pre-analysis plan or registered trial?
Provide details if
mentioned.}\label{f02-is-there-a-pre-analysis-plan-or-registered-trial-provide-details-if-mentioned}

\begin{itemize}
\tightlist
\item
  Category: \texttt{F)\ Replication\ and\ transparency}
\item
  Confidence: \texttt{low} (0.22306541673272023)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

No. The provided excerpts do not mention any pre-analysis plan or
registered trial; they describe a methodological study with simulations,
not a registered study.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The simulation experiment has two separate parts, the first for one
changepoint and the second for two changepoints
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=275-556, section=4. SIMULATION EXPERIMENT,
    note=Describes simulation setup; no registration or pre-analysis
    plan referenced.
  \end{itemize}
\end{itemize}

\subsubsection{F03: Are data access constraints disclosed (restricted
access, proprietary data,
NDAs)?}\label{f03-are-data-access-constraints-disclosed-restricted-access-proprietary-data-ndas}

\begin{itemize}
\tightlist
\item
  Category: \texttt{F)\ Replication\ and\ transparency}
\item
  Confidence: \texttt{low} (0.18228175986289236)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Terms\ of\ Use\ notice\ in\ article\ front\ matter}
\item
  Data source: JSTOR Terms and Conditions of Use notice (page 1)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. The article's front matter discloses restricted-use terms via
JSTOR's Terms and Conditions (personal, non-commercial use only; no
downloading entire issues or multiple copies without prior permission).
No NDAs are mentioned.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
you may use content in the JSTOR archive only for your personal,
non-commercial use.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=1, words=0-230, section=JSTOR Terms and Conditions notice,
    note=States personal, non-commercial use only and prohibits
    downloading entire issues or multiple copies without prior
    permission.
  \end{itemize}
\end{itemize}

\subsubsection{F04: Are key steps in data cleaning and construction
documented?}\label{f04-are-key-steps-in-data-cleaning-and-construction-documented}

\begin{itemize}
\tightlist
\item
  Category: \texttt{F)\ Replication\ and\ transparency}
\item
  Confidence: \texttt{low} (0.2456742970693667)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\ evidence}
\item
  Data source: IBM stock prices as reported by Box and Jenkins (1976)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes---key construction steps are documented (e.g., transforming IBM
prices to first differences of logs and detailed simulation setups).
Explicit data-cleaning is limited to advising outlier checks.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices...
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=275-624, section=3.1 Example: IBM Stock Prices,
    note=Documents construction of the empirical series as first
    differences of log IBM prices.
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=Details
    parameters for constructing simulated data (lengths, changepoint
    locations, variance ratios).
  \item
    page=6, words=0-349, section=ICSS Algorithm discussion, note=Advises
    complementing variance-change search with outlier detection, a
    cleaning consideration.
  \end{itemize}
\end{itemize}

\subsubsection{F05: Are robustness and sensitivity analyses fully
reported or partially
omitted?}\label{f05-are-robustness-and-sensitivity-analyses-fully-reported-or-partially-omitted}

\begin{itemize}
\tightlist
\item
  Category: \texttt{F)\ Replication\ and\ transparency}
\item
  Confidence: \texttt{low} (0.309728123445472)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Partially omitted

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The posterior odds ratio was not obtained for series with T = 500,
because it would have required a total of 1,358 CPU hours.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=9, words=200-340, section=4.3 Results of Part Two,
    note=Posterior odds not obtained for T=500; reduced replicates for
    T=200.
  \item
    page=9, words=700-780, section=4.5 The ICSS Algorithm Applied to
    Residuals, note=Indicates further work pending, suggesting
    incomplete reporting.
  \end{itemize}
\end{itemize}

\subsubsection{G01: What populations or settings are most likely to
generalize from this
study?}\label{g01-what-populations-or-settings-are-most-likely-to-generalize-from-this-study}

\begin{itemize}
\tightlist
\item
  Category: \texttt{G)\ External\ validity\ and\ generalization}
\item
  Confidence: \texttt{low} (0.21142405836654996)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Direct\ text\ statements\ on\ scope/assumptions\ and\ simulation\ design}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Sequences of independent (or prewhitened/uncorrelated) observations with
piecewise-constant variance---especially financial time series---with
moderate-to-large sample sizes (about 200+ observations). Theory and
simulations assume i.i.d. Normal innovations; autocorrelated series can
be handled by applying ICSS to AR-model residuals.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
We study the detection of multiple changes of variance in a sequence of
independent observations. There are series, particularly in the area of
finance,
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction, note=Scope: independent
    observations; finance context; moderate size (200+) applicability.
  \item
    page=2, words=550-899, section=Centered Cumulative Sums of Squares,
    note=Defines method for uncorrelated mean-zero series with
    time-varying variances.
  \item
    page=5, words=0-349, section=Asymptotics (Brownian bridge),
    note=Assumes i.i.d. Normal innovations for asymptotic distribution.
  \item
    page=9, words=550-899, section=ICSS Applied to Residuals,
    note=Application to AR(1) residuals indicates use with
    autocorrelated series after prewhitening.
  \end{itemize}
\end{itemize}

\subsubsection{G02: What populations or settings are least likely to
generalize?}\label{g02-what-populations-or-settings-are-least-likely-to-generalize}

\begin{itemize}
\tightlist
\item
  Category: \texttt{G)\ External\ validity\ and\ generalization}
\item
  Confidence: \texttt{low} (0.20618870169803688)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Simulation\ results\ and\ theoretical\ properties}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Settings least likely to generalize are short series with small variance
changes occurring near the ends, and series with two changepoints where
the variance increases monotonically. The max\textbar Dk\textbar{}
criterion is biased toward the middle, further reducing reliability for
edge changes.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
it is hard to detect a small variance change (variance ratio A =
2)---particularly when it appears at the beginning of the series
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=9, words=550-899, section=4.2 Results of Part One: Series With
    One Changepoint, note=Hard to detect small change (A=2) in short
    series, especially at the beginning.
  \item
    page=9, words=0-349, section=4.3 Results of Part Two: Series With
    Two Changepoints, note=Most difficult when variances increase
    monotonically; needs large T (\textasciitilde500).
  \item
    page=9, words=275-624, section=4.4 Sampling Distribution of k*,
    note=k* biased toward the middle of the series.
  \item
    page=4, words=275-624, section=2.3 Asymptotic Behavior of Dk Under
    Homogeneous Variance, note=max\textbar Dk\textbar{} puts more weight
    near the middle, inducing bias toward T/2.
  \end{itemize}
\end{itemize}

\subsubsection{G03: Do the authors discuss boundary conditions or scope
limits?}\label{g03-do-the-authors-discuss-boundary-conditions-or-scope-limits}

\begin{itemize}
\tightlist
\item
  Category: \texttt{G)\ External\ validity\ and\ generalization}
\item
  Confidence: \texttt{low} (0.2617001163362451)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{explicit\ text}
\item
  Table/Figure: \texttt{Table\ 1;\ Figure\ 2}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. They define statistical boundaries for Dk from its asymptotic
distribution and use a critical value D* to decide changes; they also
limit iterations and set a convergence rule (within two observations).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
These boundaries can be obtained from the asymptotic distribution of Dk
assuming constant variance.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=275-624, section=Main text on Dk plots and boundaries,
    note=Boundaries from asymptotic distribution of Dk assuming constant
    variance
  \item
    page=5, words=0-349, section=ICSS Algorithm steps, note=Decision
    rule using critical value D*; stopping condition
  \item
    page=5, words=825-1174, section=ICSS implementation details,
    note=Controls to avoid indefinite cycling; iteration limit mentioned
  \item
    page=6, words=550-899, section=Convergence criteria,
    note=Convergence when change points move by no more than two
    observations
  \item
    page=4, words=550-869, section=Asymptotic boundaries illustration,
    note=Reference to asymptotic boundaries (e.g., 1.358) shown in plots
  \end{itemize}
\end{itemize}

\subsubsection{G04: How might the results change in different time
periods or
markets?}\label{g04-how-might-the-results-change-in-different-time-periods-or-markets}

\begin{itemize}
\tightlist
\item
  Category: \texttt{G)\ External\ validity\ and\ generalization}
\item
  Confidence: \texttt{medium} (0.37517108644342595)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interprets different time periods or markets as time
  series that differ in sample length, spacing and configuration of
  variance shifts, variance ratios, outliers, and autocorrelation;
  conclusions follow the stated simulation and algorithm behavior.
\item
  Answer:
\end{itemize}

Results would vary with the series' characteristics. Detection
performance depends on sample size, spacing and configuration of
variance changes, variance ratios, and data idiosyncrasies. Longer
samples improve identification and localization; equal spacing between
change points tends to perform best; monotone variance increases are
hardest (often requiring many more observations). Performance improves
as variance ratios increase. Multiple change points can mask each other,
but the iterative ICSS procedure mitigates this. Outliers can create
false peaks and should be checked. Applying ICSS to autoregressive
residuals yields results similar to independent data. Hence, different
time periods or markets---if they differ in length, volatility regime
patterns, outlier prevalence, or autocorrelation---can produce different
numbers and locations of detected variance changes and different
accuracy levels.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The most difficult situation is when the variances change in a monotone
way; that is, the variance increases at the first change point and
increases again at the second change point.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=9, words=0-349, section=Simulation results (two changepoints),
    note=Hardest when variances change monotonically; need larger T
    (e.g., 500); best when spacing is (.33,.67) and large variance in
    the middle; ICSS often better with T ≥ 200.
  \item
    page=9, words=275-624, section=Sampling distribution of k*, note=k*
    biased toward middle; concentration around true changepoint
    increases with sample size and variance ratio.
  \item
    page=4, words=0-349, section=Behavior of E{[}Dk{]} with multiple
    changepoints, note=Piecewise linear E{[}Dk{]}; masking effect when
    \textgreater1 changepoint; dependence on configuration of variances.
  \item
    page=6, words=0-349, section=Practical considerations, note=Outliers
    can create significant Dk peaks; recommend outlier detection; ICSS
    on AR residuals performs similarly to independent data.
  \item
    page=6, words=275-624, section=Example and convergence, note=Applied
    example on IBM returns; illustrates practical variance-change
    detection in a market series.
  \end{itemize}
\end{itemize}

\subsubsection{H01: Are key variables measured directly or via
proxies?}\label{h01-are-key-variables-measured-directly-or-via-proxies}

\begin{itemize}
\tightlist
\item
  Category: \texttt{H)\ Measurement\ validity}
\item
  Confidence: \texttt{low} (0.2784010202574155)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interpreted \textquotesingle key
  variables\textquotesingle{} as variance changes/changepoints;
  underlying series values (e.g., IBM price log-differences) are
  directly observed, but variance changes are inferred.
\item
  Answer:
\end{itemize}

Proxies/inferred. The key quantities---variance changes and
changepoints---are not observed directly; they are inferred from the
data using statistics like max\textbar Dk\textbar{} (and LR), while the
raw series (e.g., IBM price log-differences) are directly observed.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This behavior leads to the search for a variance change point via maxk I
Dk I ... If this maximum absolute value exceeds a predetermined
boundary, then we may conclude that there is a change point
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=3, words=550-882, section=Detection via Dk, note=Variance
    change points concluded from the statistic max\textbar Dk\textbar{}
    crossing a boundary.
  \item
    page=6, words=275-624, section=IBM example data, note=Raw data
    described as first differences of log IBM closing prices (directly
    observed then transformed).
  \end{itemize}
\end{itemize}

\subsubsection{H02: What measurement error risks are acknowledged or
likely?}\label{h02-what-measurement-error-risks-are-acknowledged-or-likely}

\begin{itemize}
\tightlist
\item
  Category: \texttt{H)\ Measurement\ validity}
\item
  Confidence: \texttt{low} (0.2677151175224959)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\_evidence}
\item
  Table/Figure: \texttt{Table\ 2}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Inference that autocorrelation in raw series may
  distort detection is based on authors' application of ICSS to AR(1)
  residuals rather than an explicit statement of risk.
\item
  Answer:
\end{itemize}

Acknowledged/likely risks include: (1) miscalibrated thresholds from
asymptotic approximations, yielding conservative type I error and
potential under-detection; (2) biased location estimates of the change
point (k* skewed toward the series middle); (3) low power for small
samples or small variance ratios, especially when the change occurs
early, and in monotone-increasing variance scenarios, which may require
large T to detect multiple changes; (4) dependence in data can affect
detection, prompting use on AR(1) residuals rather than raw series.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This distribution is skewed; k* is biased towards the middle of the
series.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=500-610, section=Asymptotics and empirical quantiles,
    note=Empirical 95th quantile below asymptotic 1.358 implying
    conservative type I error
  \item
    page=9, words=210-270, section=4.4 Sampling Distribution of k*,
    note=k* biased toward the middle
  \item
    page=9, words=550-760, section=Results and detection difficulty,
    note=Hard to detect small variance change, especially early in
    series
  \item
    page=9, words=0-120, section=Two changepoints results, note=Most
    difficult when variances increase monotonically; need large T
  \item
    page=8, words=120-200, section=Likelihood ratio testing,
    note=Critical values must be obtained by simulation; non-regular
    problem
  \item
    page=9, words=760-899, section=AR(1) residuals application,
    note=Applied ICSS to AR(1) residuals, indicating concern about
    dependence
  \end{itemize}
\end{itemize}

\subsubsection{H03: Are there validation checks for key
measures?}\label{h03-are-there-validation-checks-for-key-measures}

\begin{itemize}
\tightlist
\item
  Category: \texttt{H)\ Measurement\ validity}
\item
  Confidence: \texttt{low} (0.2598324384323857)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{asymptotic\ theory;\ simulation\ benchmarks;\ algorithmic\ convergence\ checks;\ LR\ standardization}
\item
  Table/Figure:
  \texttt{Table\ 1\ (Empirical\ vs\ asymptotic\ quantiles\ of\ max\textbar{}Dk\textbar{});\ Table\ 2\ (percent\ below\ 1.358);\ Table\ 3\ (LR\ standardization)}
\item
  Data source: 10,000 replicates of T independent N(0,1) series (noted
  in Tables 1--2)
\item
  Answer:
\end{itemize}

Yes. The paper validates key measures by (1) deriving asymptotic
reference values for max\textbar Dk\textbar{} from a Brownian bridge and
(2) benchmarking them with large-scale simulations (Tables 1--2, with
SEs). It also standardizes likelihood-ratio tests to an extreme-value
distribution (Table 3) and uses algorithmic stopping and convergence
checks for detected changepoints.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Table 1 compares selected asymptotic quantiles of max, I D, I with
corresponding em- pirical quantiles for various series lengths obtained
by sim- ulation.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=0-349, section=Brownian bridge asymptotics; empirical
    vs asymptotic quantiles (Table 1), note=Asymptotic distribution of
    Dk and comparison to simulations
  \item
    page=3, words=550-882, section=Table 1 details and simulation note,
    note=Empirical and asymptotic quantiles of max\textbar Dk\textbar;
    10,000 replicates; Brownian-bridge reference
  \item
    page=5, words=275-624, section=Table 2 percentages and SE formula,
    note=Percentage of series with max\textbar Dk\textbar{} \textless{}
    1.358; standard errors from simulations
  \item
    page=6, words=550-899, section=Algorithm convergence checks (Step
    3), note=Convergence when number of points stabilizes and locations
    move by ≤2
  \item
    page=8, words=0-329, section=LR test standardization (Table 3) and
    extreme value reference, note=LR statistics standardized for
    reference to extreme value distribution; simulation for critical
    values
  \end{itemize}
\end{itemize}

\subsubsection{H04: Do the authors discuss construct validity for core
outcomes?}\label{h04-do-the-authors-discuss-construct-validity-for-core-outcomes}

\begin{itemize}
\tightlist
\item
  Category: \texttt{H)\ Measurement\ validity}
\item
  Confidence: \texttt{low} (0.21229758667580595)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{no\ direct\ mention\ in\ provided\ context}
\item
  Data source: Inclán \& Tiao (1994) Use of Cumulative Sums of Squares
  for Retrospective Detection of Changes of Variance, JASA
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Conclusion is limited to the provided excerpts;
  other parts of the full article might address different topics.
\item
  Answer:
\end{itemize}

No. In the provided excerpts, the authors focus on detecting variance
change points using the ICSS algorithm and related simulations; there is
no discussion of construct validity for core outcomes.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This article studies the problem of multiple change points in the
variance of a sequence of independent observations. We propose a
procedure to detect variance changes based on an ICSS algorithm.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=0-349, section=Introduction/Abstract, note=States
    study scope on variance change points and ICSS; no mention of
    construct validity or core outcomes.
  \item
    page=2, words=275-624, section=Introduction/Organization,
    note=Outlines Dk function, relation to F statistic, ICSS algorithm,
    and simulations; no construct validity discussion.
  \item
    page=7, words=0-349, section=Simulation experiment setup,
    note=Describes simulation design and performance comparisons; no
    core outcomes or validity constructs.
  \end{itemize}
\end{itemize}

\subsubsection{I01: What policy counterfactuals are considered or
implied?}\label{i01-what-policy-counterfactuals-are-considered-or-implied}

\begin{itemize}
\tightlist
\item
  Category: \texttt{I)\ Policy\ counterfactuals\ and\ welfare}
\item
  Confidence: \texttt{low} (0.2852302375069697)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual\ (scope/absence)}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

None. The provided text focuses on statistical methods (ICSS, likelihood
ratio, Bayesian) for detecting variance changes, simulations, and an IBM
stock price example; it does not consider or imply any policy
counterfactuals.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The main interest is to study the variance of a given sequence of
observations retrospectively, so we can use all the information on the
series to indicate the points of variance change.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=560-620, section=Centered Cumulative Sums of Squares,
    note=States the methodological focus on detecting variance changes
    retrospectively.
  \item
    page=7, words=20-120, section=4. SIMULATION EXPERIMENT,
    note=Describes simulation to compare procedures, not policy
    evaluation.
  \item
    page=6, words=300-380, section=3.1 Example: IBM Stock Prices,
    note=Application example is stock prices, not a policy setting.
  \end{itemize}
\end{itemize}

\subsubsection{I02: What are the main welfare tradeoffs or
distributional impacts
discussed?}\label{i02-what-are-the-main-welfare-tradeoffs-or-distributional-impacts-discussed}

\begin{itemize}
\tightlist
\item
  Category: \texttt{I)\ Policy\ counterfactuals\ and\ welfare}
\item
  Confidence: \texttt{low} (0.24546938630512277)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{absence\_in\_context}
\item
  Data source: Inclán and Tiao (1994), Journal of the American
  Statistical Association excerpt
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

None. The provided context is a methodological/statistical discussion
(ICSS algorithm, likelihood ratios, sampling distributions, and CPU-time
costs). It does not address welfare tradeoffs or socioeconomic
distributional impacts.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
the main way is by the number of "correct identifications." Another
aspect to consider is the computational requirements of each approach,
which can be measured in terms of the CPU time.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=Focuses
    on performance metrics and computational requirements, not welfare
    or distributional impacts.
  \item
    page=2, words=550-899, section=2. CENTERED CUMULATIVE SUMS OF
    SQUARES, note=Mentions measuring cost in CPU time; methodological
    emphasis.
  \item
    page=9, words=550-899, section=4.2 Results of Part One,
    note=Discusses detection performance and computational burden; no
    welfare analysis.
  \end{itemize}
\end{itemize}

\subsubsection{I03: Are cost-benefit or incidence analyses
provided?}\label{i03-are-cost-benefit-or-incidence-analyses-provided}

\begin{itemize}
\tightlist
\item
  Category: \texttt{I)\ Policy\ counterfactuals\ and\ welfare}
\item
  Confidence: \texttt{low} (0.28150735669307536)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{textual\ (CPU\ time\ comparison;\ simulation\ summaries)}
\item
  Data source: Provided article text (Sections 4 and 4.6)
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interprets `cost-benefit' as an economic evaluation
  and `incidence analyses' as occurrence rates of outcomes; neither is
  presented---only computational cost (CPU time) and simulation
  frequencies.
\item
  Answer:
\end{itemize}

No. The paper reports computational cost comparisons (CPU time) and
simulation frequencies but does not provide cost-benefit or incidence
analyses.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Another aspect to consider is the computational requirements of each
approach, which can be measured in terms of the CPU time.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=2, words=550-899, section=Introduction/Background, note=Cost
    framed explicitly as CPU time for comparing approaches.
  \item
    page=7, words=0-349, section=4. SIMULATION EXPERIMENT, note=States
    computational requirements measured in CPU time.
  \item
    page=9, words=550-899, section=4.6 CPU Time Requirements,
    note=Details on CPU time burden and comparisons.
  \item
    page=11, words=0-349, section=CPU time regression summary,
    note=Regression models summarizing CPU time; recommendation based on
    computational cost.
  \end{itemize}
\end{itemize}

\subsubsection{I04: What policy recommendations are stated or
implied?}\label{i04-what-policy-recommendations-are-stated-or-implied}

\begin{itemize}
\tightlist
\item
  Category: \texttt{I)\ Policy\ counterfactuals\ and\ welfare}
\item
  Confidence: \texttt{low} (0.21524732197355106)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association, 89(427), 913--923.
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interprets ``policy recommendations'' as
  methodological/practical guidance for applying variance-change
  detection procedures; no public policy is discussed.
\item
  Answer:
\end{itemize}

No explicit public policy is given; the article offers methodological
recommendations: (1) Use the ICSS algorithm for detecting variance
changes, especially when computational burden is a concern. (2) Control
ICSS iterations to avoid cycling (cap iterations; declare convergence
when change-point locations move by \textless2 observations). (3) After
detecting change points, estimate segment variances to build a
homogeneous ``corrected series'' and compute residual diagnostics (e.g.,
Q statistic) on that corrected series. (4) For likelihood-ratio testing,
proceed sequentially with LR\_\{n-1,n\}, obtain critical values by
simulation, and standardize to an extreme value distribution. (5) For
Bayesian inference, compute posterior-odds K\_\{n,n-1\} up to a
specified maximum number of changes Nmax and choose NT maximizing the
conditional probability. (6) For time series with dynamics, apply ICSS
to residuals from fitted AR models. (7) Practical power guidance: small
samples (T≈100) and small variance ratios (A=2), especially with early
changes, are hard to detect; performance improves notably with longer
series (T≥200) or larger variance ratios (A=3).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This "corrected series" should then be used to obtain other residual
diagnostics, like the Q statistic, which would be distorted if
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=825-1174, section=ICSS algorithm and implementation,
    note=Recommends iteration controls to avoid cycling; mentions
    practical cap (\textasciitilde20 iterations).
  \item
    page=6, words=550-899, section=ICSS convergence and post-detection
    use, note=Convergence rule (\textless2 observations movement) and
    recommendation to create a corrected series for subsequent
    diagnostics.
  \item
    page=8, words=0-329, section=Likelihood ratio strategy,
    note=Sequential LR\_\{n-1,n\}; critical values by simulation;
    extreme value standardization.
  \item
    page=9, words=275-624, section=Bayesian posterior odds procedure,
    note=Systematic use of posterior odds with specified Nmax; choose NT
    maximizing conditional probability.
  \item
    page=9, words=550-899, section=Performance and computational
    guidance, note=ICSS for computational efficiency; detection
    difficulty at T=100 and A=2; improved performance with T≥200 or A=3;
    apply ICSS to AR residuals.
  \end{itemize}
\end{itemize}

\subsubsection{J01: What missingness or attrition patterns are
reported?}\label{j01-what-missingness-or-attrition-patterns-are-reported}

\begin{itemize}
\tightlist
\item
  Category: \texttt{J)\ Data\ quality\ and\ integrity}
\item
  Confidence: \texttt{low} (0.304980734681012)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Data source: Simulation experiment description and results text
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Interprets
  \textquotesingle missingness/attrition\textquotesingle{} as data loss
  or dropouts; none reported. Only computational omissions in
  posterior-odds analyses are noted.
\item
  Answer:
\end{itemize}

No data missingness or attrition is reported. All simulated series were
complete of length T. The only omission was analytical: posterior odds
were not computed for T=500, and for T=200 only 100 (vs 1,000)
replicates were run.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The posterior odds ratio was not obtained for series with T = 500... the
runs with T = 200 for the log posterior odds have only 100 replicates
instead of the 1,000...
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=7, words=275-556, section=Simulation experiment methods,
    note=States each replicate is a full series of length T of
    independent N(0,1) variables.
  \item
    page=9, words=0-349, section=Results/notes on computational
    feasibility, note=Explains posterior odds not obtained for T=500 and
    only 100 replicates for T=200.
  \end{itemize}
\end{itemize}

\subsubsection{J02: How are outliers handled (winsorization, trimming,
exclusions)?}\label{j02-how-are-outliers-handled-winsorization-trimming-exclusions}

\begin{itemize}
\tightlist
\item
  Category: \texttt{J)\ Data\ quality\ and\ integrity}
\item
  Confidence: \texttt{low} (0.3496926880881717)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Outliers are not handled by winsorization or trimming. Instead, users
should visually inspect Dk plots---outliers appear as sudden jumps
without a change in slope---and complement the variance-change search
with a dedicated outlier-detection procedure (e.g., Chang, Tiao, and
Chen 1988).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
a big outlier would create a significant peak in the Dk plot... it is
advisable to complement the search for variance changes with a procedure
for outlier detection
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=0-349, section=Section 3 (ICSS algorithm guidance),
    note=Outliers create significant peaks; use outlier detection
    alongside ICSS.
  \end{itemize}
\end{itemize}

\subsubsection{J03: Are there data audits or validation steps
described?}\label{j03-are-there-data-audits-or-validation-steps-described}

\begin{itemize}
\tightlist
\item
  Category: \texttt{J)\ Data\ quality\ and\ integrity}
\item
  Confidence: \texttt{low} (0.2451173391958245)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Algorithm\ description\ and\ simulation\ study\ reported\ in\ the\ text}
\item
  Table/Figure: \texttt{Table\ 1\ and\ Table\ 2}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. The ICSS procedure includes internal validation checks of detected
changepoints (re-checking each candidate given adjacent ones and
repeating until convergence with a defined tolerance), iteration
controls to prevent cycling, and external validation via simulation
(empirical quantile comparisons, standard errors, and a simulation study
assessing correct identifications and CPU time).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
Check each possible changepoint by calculating
Dk(a{[}cpj−1+1:cpj+1{]})\ldots{} Repeat Step 3 until the number of
changepoints does not change\ldots{} we consider that if each
changepoint is within two observations\ldots{} then the algorithm has
converged.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=5, words=550-899, section=ICSS Algorithm Step 3 (validation and
    convergence), note=Re-check each candidate changepoint; repeat until
    number/locations stabilize within tolerance.
  \item
    page=5, words=825-1174, section=ICSS implementation safeguards,
    note=Controls over number of iterations to avoid cycling; practical
    convergence behavior.
  \item
    page=5, words=0-349, section=Asymptotic vs empirical quantiles,
    note=Table 1 compares asymptotic and empirical quantiles for max
    \textbar Dk\textbar{} via simulation.
  \item
    page=5, words=275-624, section=Empirical validation and Type I
    error, note=Percentages with standard errors; empirical 95th
    quantile lower than asymptotic value.
  \item
    page=7, words=0-349, section=Simulation experiment design,
    note=Simulation to compare ICSS with alternatives; uses correctness
    and CPU time as metrics.
  \end{itemize}
\end{itemize}

\subsubsection{J04: Is there evidence of reporting bias or selective
sample
inclusion?}\label{j04-is-there-evidence-of-reporting-bias-or-selective-sample-inclusion}

\begin{itemize}
\tightlist
\item
  Category: \texttt{J)\ Data\ quality\ and\ integrity}
\item
  Confidence: \texttt{low} (0.2814155547201351)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Textual\ evidence\ from\ methods/results\ and\ table\ notes}
\item
  Table/Figure: \texttt{Table\ 4\ note;\ Section\ 4\ design\ notes}
\item
  Data source: Journal article text (JASA 1994)
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

No. The paper transparently describes its simulation design, notes when
certain results were not computed due to CPU-time limits, and explains
the elimination of redundant variance permutations by symmetry---none of
which indicate reporting bias or selective sample inclusion.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The posterior odds ratio was not obtained for series with T = 500,
because it would have required a total of 1,358 CPU hours.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=9, words=0-349, section=Section 4 (Simulation Experiment),
    results discussion, note=Explicitly states omission of
    posterior-odds results for T=500 and reduced replicates for T=200
    due to computational cost.
  \item
    page=7, words=275-556, section=Section 4 (Simulation Experiment),
    design details, note=Explains elimination of some
    variance-permutation cases based on symmetry, not selective
    inclusion.
  \item
    page=10, words=0-84, section=Table note, note=Clarifies differing
    replicate counts across conditions; transparent reporting rather
    than selective.
  \end{itemize}
\end{itemize}

\subsubsection{K01: What goodness-of-fit or diagnostic metrics are
reported?}\label{k01-what-goodness-of-fit-or-diagnostic-metrics-are-reported}

\begin{itemize}
\tightlist
\item
  Category: \texttt{K)\ Model\ fit\ and\ diagnostics}
\item
  Confidence: \texttt{low} (0.31894037148092247)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text\_and\_tables}
\item
  Table/Figure: \texttt{Table\ 4;\ Table\ 5;\ Table\ 6}
\item
  Data source: Inclán \& Tiao (1994), Journal of the American
  Statistical Association
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

They report residual diagnostics using the Q statistic (to be applied to
the variance-corrected series), frequency distributions of the number of
detected variance changes (NT) for ICSS, LR tests, and Bayesian
posterior odds, and summaries of the sampling distribution of the
estimated changepoint location k* (mode, mean, and SD), noting its
skewness and bias toward the series middle.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
other residual diagnostics, like the Q statistic, which would be
distorted if calculated
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=550-899, section=Diagnostics (residuals/Q statistic),
    note=Mentions using residual diagnostics like the Q statistic on
    corrected series.
  \item
    page=7, words=275-556, section=4.1 Assessing the Evidence,
    note=Introduces Table 4: frequency distribution of NT for one
    change.
  \item
    page=8, words=0-329, section=Two changepoints results,
    note=Introduces Table 5: frequency distribution of NT for two
    changes.
  \item
    page=9, words=0-349, section=4.4 Sampling Distribution of k*,
    note=Table 6 provides Mode of k*, Mean of k*, SD of k*; discussion
    of skew/bias.
  \item
    page=9, words=275-624, section=4.4 Sampling Distribution of k*,
    note=Explains k* distribution is skewed and biased toward the
    middle.
  \end{itemize}
\end{itemize}

\subsubsection{K02: Are functional form choices tested (logs, levels,
nonlinearities)?}\label{k02-are-functional-form-choices-tested-logs-levels-nonlinearities}

\begin{itemize}
\tightlist
\item
  Category: \texttt{K)\ Model\ fit\ and\ diagnostics}
\item
  Confidence: \texttt{low} (0.3410795504216201)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{text}
\item
  Assumption flag: \texttt{True}
\item
  Assumption notes: Conclusion based on absence of any mention of
  testing logs vs. levels or nonlinearities in the provided excerpts;
  only a log-difference transformation is described for the example.
\item
  Answer:
\end{itemize}

No. In the provided context, there is no evidence that functional-form
choices (logs vs. levels or nonlinearities) were tested. The example
simply uses first-differenced log prices and proceeds with
variance-change detection.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The series analyzed is the first difference of the logarithm of the IBM
stock closing prices
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=275-624, section=3.1 Example: IBM Stock Prices,
    note=Uses first difference of log prices; no discussion of testing
    alternative functional forms.
  \item
    page=7, words=275-556, section=4. Simulation Experiment,
    note=Simulations use independent N(0,1) series; focus is on variance
    changes, not functional-form tests.
  \end{itemize}
\end{itemize}

\subsubsection{K03: Are residual checks or specification tests
reported?}\label{k03-are-residual-checks-or-specification-tests-reported}

\begin{itemize}
\tightlist
\item
  Category: \texttt{K)\ Model\ fit\ and\ diagnostics}
\item
  Confidence: \texttt{low} (0.32987787498865473)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type: \texttt{textual}
\item
  Table/Figure: \texttt{Table\ 8}
\item
  Data source: Inclán, C., \& Tiao, G. C. (1994). Use of Cumulative Sums
  of Squares for Retrospective Detection of Changes of Variance. JASA,
  89(427), 913--923.
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Yes. The paper discusses residual diagnostics (e.g., the Q statistic)
and applies the ICSS algorithm to AR(1) residuals, reporting results in
Table 8.

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
This "corrected series" should then be used to obtain other residual
diagnostics, like the Q statistic
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=6, words=825-968, note=Mentions using residual diagnostics like
    the Q statistic after variance correction.
  \item
    page=9, words=550-899, note=States the ICSS algorithm was applied to
    AR(1) residuals and summarizes results.
  \item
    page=10, words=0-84, note=Table 8 title confirms residual-based
    results are reported.
  \end{itemize}
\end{itemize}

\subsubsection{K04: How sensitive are results to alternative
specifications or
estimators?}\label{k04-how-sensitive-are-results-to-alternative-specifications-or-estimators}

\begin{itemize}
\tightlist
\item
  Category: \texttt{K)\ Model\ fit\ and\ diagnostics}
\item
  Confidence: \texttt{medium} (0.3882154722563691)
\item
  Retrieval method: \texttt{local}
\item
  Evidence type:
  \texttt{Simulation\ comparisons,\ methodological\ notes,\ and\ algorithmic\ diagnostics}
\item
  Table/Figure: \texttt{Table\ 1;\ Tables\ 4–8}
\item
  Data source: Journal of the American Statistical Association (1994),
  Inclán and Tiao ICSS study
\item
  Assumption flag: \texttt{False}
\item
  Answer:
\end{itemize}

Results are sensitive to the chosen test/estimator, critical values, and
data-generation specifications. Method choice: For small variance
changes (A=2), the Bayesian posterior-odds approach performs best, while
ICSS improves notably with larger samples or larger variance ratios and
often outperforms likelihood-ratio (LR) tests; LR is frequently weaker
in multi-change settings. Threshold choice: Using empirical quantiles
for max\textbar Dk\textbar{} yields lower critical values than the
asymptotic 1.358, reducing type I error relative to the asymptotic
boundary, so conclusions depend on whether asymptotic or
simulation-based cutoffs are used. Model structure and iteration:
Accounting for adjacent changepoints can remove previously `significant'
candidates (e.g., point 342 becomes non-significant after neighboring
points are included), indicating sensitivity to the iterative
specification. Estimation context: Applying ICSS to AR(1) residuals
using OLS produces detection rates very close to the i.i.d. case,
suggesting limited sensitivity to estimating the AR parameter. Estimator
of location k*: its sampling distribution is skewed (biased toward the
center), but the mode is at the true change; concentration improves with
larger T or larger variance ratios. Performance also depends on
changepoint locations and variance configurations (hardest under
monotone increases; easier with equal segment lengths or when the large
variance is in the middle).

\begin{itemize}
\tightlist
\item
  Quote snippet:
\end{itemize}

\begin{quote}
The Bayesian procedure is the best for A = 2 and gives comparable
results to the other approaches with A = 3.
\end{quote}

\begin{itemize}
\tightlist
\item
  Citation anchors:

  \begin{itemize}
  \tightlist
  \item
    page=9, words=640-880, section=4.2, note=Bayesian best for A=2; ICSS
    improves with longer series/larger ratios; LR comparison
  \item
    page=5, words=470-620, section=Quantiles/Type I error,
    note=Empirical 95th quantile lower than asymptotic 1.358; smaller
    empirical type I error
  \item
    page=6, words=560-720, section=Algorithm Step 3, note=Point 342
    becomes non-significant after considering adjacent changepoints
  \item
    page=9, words=820-899, section=Residuals, note=ICSS on AR(1)
    residuals: percentages close to Table 4 (i.i.d. case)
  \item
    page=9, words=275-540, section=4.4, note=k* distribution skewed;
    biased to middle; mode at true change; concentration increases with
    T or A
  \item
    page=9, words=0-220, section=4.3, note=Hardest when variances change
    monotonically; ICSS often better than LR with T≥200; best at equal
    segment lengths or large variance in middle
  \item
    page=3, words=190-349, section=2.1, note=Distinction between F
    (known k) and searching max\textbar Dk\textbar{} (unknown k)
  \end{itemize}
\end{itemize}

\end{document}
