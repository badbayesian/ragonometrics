% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{longtable,booktabs,array}
\newcounter{none} % for unnumbered tables
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\section{Workstream Comparison: Use of Cumulative Sums of Squares (GPT-5 vs GPT-5-nano)}\label{workstream-comparison-use-of-cumulative-sums-of-squares-gpt-5-vs-gpt-5-nano}

\subsection{Inputs}\label{inputs}

\begin{itemize}
\tightlist
\item
  Baseline (archive): \texttt{reports/archived/workflow-report-1308532de7a9446d813e57129826aa71.json}
\item
  Variant (current): \texttt{reports/workflow-report-34c88b2bf0194f5b9b72793845290a52.json}
\end{itemize}

\subsection{Schema}\label{schema}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Field & Type & Purpose \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{comparison\_group\_id} & \texttt{text} & Logical key that groups runs into one composed workstream comparison. \\
\texttt{source\_bucket} & `enum(archive & current)` \\
\texttt{report\_path} & \texttt{text} & Path to the source workflow-report JSON used for this row. \\
\texttt{run\_id} & \texttt{text} & Unique workflow execution identifier. \\
\texttt{papers\_dir} & \texttt{text} & Input paper path(s) processed by the run. \\
\texttt{started\_at} & \texttt{timestamptz} & Workflow start timestamp. \\
\texttt{finished\_at} & \texttt{timestamptz} & Workflow end timestamp. \\
\texttt{duration\_sec} & \texttt{numeric} & Run duration in seconds for cost/latency comparison. \\
\texttt{chat\_model} & \texttt{text} & Primary LLM used by the run. \\
\texttt{embedding\_model} & \texttt{text} & Embedding model used for retrieval/chunk similarity. \\
\texttt{report\_question\_set} & \texttt{enum} & Question set mode (structured \\
\texttt{report\_question\_count} & \texttt{integer} & Number of structured/agentic report question records emitted. \\
\texttt{agentic\_status} & \texttt{text} & Top-level status for agentic stage. \\
\texttt{index\_status} & \texttt{text} & Top-level status for index stage. \\
\texttt{index\_reason\_or\_error} & \texttt{text} & Failure/skipped reason to diagnose differences. \\
\texttt{report\_store\_status} & \texttt{text} & Whether report was stored to Postgres. \\
\texttt{prep\_corpus\_hash} & \texttt{text} & Fingerprint of corpus content; validates same input set. \\
\texttt{config\_hash} & \texttt{text} & Effective configuration hash; validates run comparability. \\
\texttt{confidence\_mean} & \texttt{numeric} & Mean confidence score across structured questions. \\
\texttt{confidence\_labels} & \texttt{text} & Label distribution summary for quick quality profile. \\
\texttt{final\_answer\_preview} & \texttt{text} & Truncated synthesis preview for side-by-side semantic comparison. \\
\end{longtable}
}

\subsection{Records}\label{records}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}lllllllllllllllllllll@{}}
\toprule\noalign{}
comparison\_group\_id & source\_bucket & report\_path & run\_id & papers\_dir & started\_at & finished\_at & duration\_sec & chat\_model & embedding\_model & report\_question\_set & report\_question\_count & agentic\_status & index\_status & index\_reason\_or\_error & report\_store\_status & prep\_corpus\_hash & config\_hash & confidence\_mean & confidence\_labels & final\_answer\_preview \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
icss-gpt5-vs-gpt5-nano-v1 & archive & reports/archived/workflow-report-1308532de7a9446d813e57129826aa71.json & 1308532de7a9446d813e57129826aa71 & papers\textbackslash Use\_of\_Cumulative\_Sums\_of\_Squares\_for\_Re.pdf & 2026-02-15T17:40:01.994514+00:00 & 2026-02-15T17:53:00.314525+00:00 & 778.32 & gpt-5 & text-embedding-3-large & structured & 83 & completed & skipped & db\_unreachable & skipped & 5f8df89d3695858097ecae0e151ea277f490e3ad831113e01010c76e2243a47e & ed36f9c374bd26f642bca2aef6a1df3ce5139b582ea5b545619c7caf2071f355 & 0.27249828745446014 & high=0, medium=6, low=77 & - Primary: Introduces the ICSS procedure for variance change-point detection---built on a statistic \(D_k\) that is a monotone transform of the two-sample variance F statistic \(F_{T-k,k}=\frac{(C_T-C_k)/(T-k)}{C_k/k}\) with \$C\_k=\textbackslash sum\_\{t=1\}\^{}k ... \\
icss-gpt5-vs-gpt5-nano-v1 & current & reports/workflow-report-34c88b2bf0194f5b9b72793845290a52.json & 34c88b2bf0194f5b9b72793845290a52 & papers\textbackslash Use\_of\_Cumulative\_Sums\_of\_Squares\_for\_Re.pdf & 2026-02-15T21:46:06.642379+00:00 & 2026-02-15T21:52:34.727145+00:00 & 388.08 & gpt-5-nano & text-embedding-3-large & structured & 83 & completed & indexed & & stored & 5f8df89d3695858097ecae0e151ea277f490e3ad831113e01010c76e2243a47e & 8d63faf73797db1e19a30727d3482d381e1228b44649dd760b815556e9b3b8b1 & 0.2724441085027672 & high=0, medium=6, low=77 & - Key contribution - Introduces ICSS (Cumulative Sums of Squares) as a fast, retrospective detector of variance changes in time series, using the D\_k statistic to flag potential change points. (Incl√°n \& Tiao, page 3, words 0-349) - Co... \\
\end{longtable}
}

\subsection{Comparison Notes}\label{comparison-notes}

\begin{itemize}
\tightlist
\item
  Both runs process the same paper path: \texttt{papers\textbackslash{}Use\_of\_Cumulative\_Sums\_of\_Squares\_for\_Re.pdf}.
\item
  Baseline model: \texttt{gpt-5}; variant model: \texttt{gpt-5-nano}.
\item
  Baseline duration: \texttt{778.32} sec; variant duration: \texttt{388.08} sec.
\item
  Baseline confidence mean: \texttt{0.27249828745446014}; variant confidence mean: \texttt{0.2724441085027672}.
\item
  Baseline report-question count: \texttt{83}; variant report-question count: \texttt{83}.
\item
  Baseline index status: \texttt{skipped} (db\_unreachable); variant index status: \texttt{indexed} (None).
\item
  Baseline report store: \texttt{skipped}; variant report store: \texttt{stored}.
\end{itemize}

\end{document}
