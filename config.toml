[ragonometrics]
# Primary config file. Env vars override any values here.

# Ingestion and chunking
papers_dir = "papers"
max_papers = 6
max_words = 30000
chunk_words = 350
chunk_overlap = 75

# Retrieval
top_k = 10
bm25_weight = 0.45
reranker_model = "gpt-5-nano"
rerank_top_n = 30
query_expansion = "1"
query_expand_model = "gpt-5-nano"

# Embeddings/LLM
batch_size = 64
embedding_model = "text-embedding-3-large"
chat_model = "gpt-5-nano"

# Provider routing (optional; defaults preserve OpenAI behavior)
llm_provider = "openai" # openai | anthropic | openai_compatible
# llm_base_url = "http://localhost:11434/v1" # for openai_compatible providers (Ollama/vLLM/etc.)
# llm_api_key = "" # optional generic key for openai_compatible endpoints
# openai_api_key = ""
# anthropic_api_key = ""
# openai_compatible_api_key = ""
# chat_provider = ""
# embedding_provider = ""
# rerank_provider = ""
# query_expand_provider = ""
# metadata_title_provider = ""
# chat_provider_fallback = ""
# embedding_provider_fallback = ""
# rerank_provider_fallback = ""
# query_expand_provider_fallback = ""
# metadata_title_provider_fallback = ""
# metadata_title_model = ""

# Storage / indexing
database_url = "postgres://postgres:postgres@localhost:5432/ragonometrics"
index_idempotent_skip = true
allow_unverified_index = false

# Extraction options
section_aware_chunking = true
force_ocr = false
